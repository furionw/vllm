[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:31:41 [api_server.py:872] vLLM API server version 0.1.dev13160+gf62857ce3
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:31:41 [utils.py:267] non-default args: {'model_tag': 'Qwen/Qwen3-VL-8B-Instruct', 'model': 'Qwen/Qwen3-VL-8B-Instruct', 'trust_remote_code': True, 'max_model_len': 12000, 'enable_prefix_caching': False, 'limit_mm_per_prompt': {'image': 2}, 'encoder_cache_size': 400, 'enable_log_requests': True}
[0;36m(APIServer pid=2387499)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[0;36m(APIServer pid=2387499)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:31:42 [model.py:541] Resolved architecture: Qwen3VLForConditionalGeneration
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:31:42 [model.py:1559] Using max model len 12000
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:31:42 [scheduler.py:232] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:31:42 [vllm.py:618] Asynchronous scheduling is enabled.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:31:46 [core.py:96] Initializing a V1 LLM engine (v0.1.dev13160+gf62857ce3) with config: model='Qwen/Qwen3-VL-8B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen3-VL-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=12000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen3-VL-8B-Instruct, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer', 'vllm::rocm_aiter_sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:31:47 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://192.168.86.26:36305 backend=nccl
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:31:47 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:31:52 [gpu_model_runner.py:3835] Starting to load model Qwen/Qwen3-VL-8B-Instruct...
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:31:52 [mm_encoder_attention.py:86] Using AttentionBackendEnum.FLASH_ATTN for MMEncoderAttention.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:31:52 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=2387706)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=2387706)[0;0m Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.79it/s]
[0;36m(EngineCore_DP0 pid=2387706)[0;0m Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.51it/s]
[0;36m(EngineCore_DP0 pid=2387706)[0;0m Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.47it/s]
[0;36m(EngineCore_DP0 pid=2387706)[0;0m Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.66it/s]
[0;36m(EngineCore_DP0 pid=2387706)[0;0m Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.61it/s]
[0;36m(EngineCore_DP0 pid=2387706)[0;0m 
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:31:56 [default_loader.py:291] Loading weights took 2.59 seconds
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:31:56 [gpu_model_runner.py:3932] Model loading took 16.64 GiB memory and 3.388328 seconds
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:31:56 [gpu_model_runner.py:4750] Encoder cache will be initialized with a budget of 400 tokens, and profiled with 1 image items of the maximum feature size.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:01 [backends.py:805] Using cache directory: /home/qiwa/.cache/vllm/torch_compile_cache/7d40dc110d/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:01 [backends.py:865] Dynamo bytecode transform time: 3.95 s
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:04 [backends.py:302] Cache the graph of compile range (1, 2048) for later use
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:05 [backends.py:319] Compiling a graph for compile range (1, 2048) takes 1.61 s
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:05 [monitor.py:34] torch.compile takes 5.56 s in total
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:06 [gpu_worker.py:355] Available KV cache memory: 23.69 GiB
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:06 [kv_cache_utils.py:1307] GPU KV cache size: 172,496 tokens
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:06 [kv_cache_utils.py:1312] Maximum concurrency for 12,000 tokens per request: 14.37x
[0;36m(EngineCore_DP0 pid=2387706)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|â–         | 2/51 [00:00<00:02, 18.86it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|â–Š         | 4/51 [00:00<00:02, 19.01it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|â–ˆâ–        | 6/51 [00:00<00:02, 18.34it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|â–ˆâ–Œ        | 8/51 [00:00<00:02, 18.49it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|â–ˆâ–ˆâ–       | 11/51 [00:00<00:01, 20.05it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|â–ˆâ–ˆâ–‹       | 14/51 [00:00<00:01, 20.91it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 17/51 [00:00<00:01, 22.03it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 20/51 [00:00<00:01, 23.72it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/51 [00:01<00:01, 25.00it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 26/51 [00:01<00:00, 25.92it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 29/51 [00:01<00:00, 26.58it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/51 [00:01<00:00, 27.92it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 37/51 [00:01<00:00, 28.96it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 40/51 [00:01<00:00, 28.91it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 44/51 [00:01<00:00, 29.94it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 48/51 [00:01<00:00, 30.95it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:01<00:00, 26.09it/s]
[0;36m(EngineCore_DP0 pid=2387706)[0;0m Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graphs (decode, FULL):   9%|â–Š         | 3/35 [00:00<00:01, 24.11it/s]Capturing CUDA graphs (decode, FULL):  17%|â–ˆâ–‹        | 6/35 [00:00<00:01, 25.68it/s]Capturing CUDA graphs (decode, FULL):  26%|â–ˆâ–ˆâ–Œ       | 9/35 [00:00<00:00, 26.75it/s]Capturing CUDA graphs (decode, FULL):  34%|â–ˆâ–ˆâ–ˆâ–      | 12/35 [00:00<00:00, 27.86it/s]Capturing CUDA graphs (decode, FULL):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 16/35 [00:00<00:00, 29.33it/s]Capturing CUDA graphs (decode, FULL):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 20/35 [00:00<00:00, 30.32it/s]Capturing CUDA graphs (decode, FULL):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 24/35 [00:00<00:00, 31.34it/s]Capturing CUDA graphs (decode, FULL):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 28/35 [00:00<00:00, 32.15it/s]Capturing CUDA graphs (decode, FULL):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 32/35 [00:01<00:00, 33.35it/s]Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:01<00:00, 31.18it/s]
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:10 [gpu_model_runner.py:4891] Graph capturing finished in 4 secs, took 0.56 GiB
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:10 [core.py:272] init engine (profile, create kv cache, warmup model) took 13.79 seconds
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:10 [vllm.py:618] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:10 [api_server.py:663] Supported tasks: ['generate']
[0;36m(APIServer pid=2387499)[0;0m WARNING 01-21 21:32:10 [model.py:1372] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:10 [serving.py:227] Using default chat sampling params from model: {'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:11 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:11 [serving.py:185] Warming up chat template processing...
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:13 [chat_utils.py:599] Detected the chat template content format to be 'openai'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:13 [serving.py:221] Chat template warmup completed in 2216.8ms
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:13 [serving.py:80] Using default completion sampling params from model: {'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:13 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:13 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:8000
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:13 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:13 [launcher.py:46] Route: /openapi.json, Methods: HEAD, GET
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:13 [launcher.py:46] Route: /docs, Methods: HEAD, GET
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:13 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: HEAD, GET
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:13 [launcher.py:46] Route: /redoc, Methods: HEAD, GET
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:13 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:13 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:13 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:13 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:13 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:13 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:13 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:13 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:13 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:13 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:13 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:13 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:13 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:13 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:13 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:13 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:13 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:13 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:13 [launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:13 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:13 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:13 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:13 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:13 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:13 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:13 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:13 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:13 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:13 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:13 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:13 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:13 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:13 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:13 [launcher.py:46] Route: /pooling, Methods: POST
[0;36m(APIServer pid=2387499)[0;0m INFO:     Started server process [2387499]
[0;36m(APIServer pid=2387499)[0;0m INFO:     Waiting for application startup.
[0;36m(APIServer pid=2387499)[0;0m INFO:     Application startup complete.
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:23 [logger.py:49] Received request chatcmpl-e4878e0a-3170-4884-8cae-1fbdc61203b5: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:27 [async_llm.py:382] Added request chatcmpl-e4878e0a-3170-4884-8cae-1fbdc61203b5-9827785d.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:27 [encoder_cache_manager.py:109] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is not cached in GPU cache or CPU cache
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:27 [encoder_cache_manager.py:194] num_embeds: 256, num_free_slots: 400, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:27 [encoder_cache_manager.py:238] Adding c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc to cached_cpu
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:27 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:27 [gpu_model_runner.py:2336] Adding c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc to encoder_cache_cpu
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:27 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:27 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:27 [logger.py:49] Received request chatcmpl-b357a6cd-37f1-4ed5-93a6-57a4dedc4db9: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:27 [async_llm.py:382] Added request chatcmpl-b357a6cd-37f1-4ed5-93a6-57a4dedc4db9-b5626e1f.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:27 [encoder_cache_manager.py:109] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is not cached in GPU cache or CPU cache
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:27 [encoder_cache_manager.py:194] num_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:27 [encoder_cache_manager.py:208] Evicting from GPU cache to free space: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:27 [encoder_cache_manager.py:238] Adding cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da to cached_cpu
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:27 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:27 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:27 [gpu_model_runner.py:2336] Adding cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da to encoder_cache_cpu
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:27 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:27 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:27 [logger.py:49] Received request chatcmpl-c1930623-8691-4029-9fab-794f0318765c: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:27 [async_llm.py:382] Added request chatcmpl-c1930623-8691-4029-9fab-794f0318765c-b3729361.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:27 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:27 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:27 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:27 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:27 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:27 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:27 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:27 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:27 [logger.py:49] Received request chatcmpl-7e33ea3a-2a52-4936-90f0-7d602831a5c7: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:27 [async_llm.py:382] Added request chatcmpl-7e33ea3a-2a52-4936-90f0-7d602831a5c7-903e734d.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:27 [encoder_cache_manager.py:109] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is not cached in GPU cache or CPU cache
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:27 [encoder_cache_manager.py:194] num_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:27 [encoder_cache_manager.py:208] Evicting from GPU cache to free space: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:27 [encoder_cache_manager.py:238] Adding 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 to cached_cpu
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:27 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:27 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:27 [gpu_model_runner.py:2336] Adding 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 to encoder_cache_cpu
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:27 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:27 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:27 [logger.py:49] Received request chatcmpl-03f7671f-be68-4b1b-8e78-fc990e2eafe0: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:27 [async_llm.py:382] Added request chatcmpl-03f7671f-be68-4b1b-8e78-fc990e2eafe0-a5e77741.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:27 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:27 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:27 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:27 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:27 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:27 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:27 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:28 [logger.py:49] Received request chatcmpl-617fb905-ff93-4586-8a9a-00b6560f754b: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:28 [async_llm.py:382] Added request chatcmpl-617fb905-ff93-4586-8a9a-00b6560f754b-a08021f5.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [encoder_cache_manager.py:109] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is not cached in GPU cache or CPU cache
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [encoder_cache_manager.py:194] num_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [encoder_cache_manager.py:208] Evicting from GPU cache to free space: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [encoder_cache_manager.py:238] Adding 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 to cached_cpu
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [gpu_model_runner.py:2336] Adding 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 to encoder_cache_cpu
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:28 [logger.py:49] Received request chatcmpl-1c529568-bb22-4498-b20f-1e42d46ed862: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:28 [async_llm.py:382] Added request chatcmpl-1c529568-bb22-4498-b20f-1e42d46ed862-8f5b3dd9.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [encoder_cache_manager.py:118] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:28 [logger.py:49] Received request chatcmpl-19540186-f2af-4df7-9be9-9b19df6b87c7: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:28 [async_llm.py:382] Added request chatcmpl-19540186-f2af-4df7-9be9-9b19df6b87c7-a3be46ac.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:28 [logger.py:49] Received request chatcmpl-779179bc-2ed9-4eb9-af54-4f61699b63b7: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:28 [async_llm.py:382] Added request chatcmpl-779179bc-2ed9-4eb9-af54-4f61699b63b7-af4c8810.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:28 [logger.py:49] Received request chatcmpl-f920d747-7109-4516-b678-29166637ec97: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:28 [async_llm.py:382] Added request chatcmpl-f920d747-7109-4516-b678-29166637ec97-b9793bdd.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:28 [logger.py:49] Received request chatcmpl-231b1b24-2779-4473-9323-57cac2df2a84: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:28 [async_llm.py:382] Added request chatcmpl-231b1b24-2779-4473-9323-57cac2df2a84-9dc9b149.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:28 [logger.py:49] Received request chatcmpl-a321c2ed-1a5c-4346-aad8-749c7ce976c5: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:28 [async_llm.py:382] Added request chatcmpl-a321c2ed-1a5c-4346-aad8-749c7ce976c5-954bbbf0.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:28 [logger.py:49] Received request chatcmpl-e9802306-7c4c-42b8-b89b-bbf405573ba7: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:28 [async_llm.py:382] Added request chatcmpl-e9802306-7c4c-42b8-b89b-bbf405573ba7-bcc9a82f.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:28 [logger.py:49] Received request chatcmpl-70142731-dce8-46a7-be11-0744a05200df: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:28 [async_llm.py:382] Added request chatcmpl-70142731-dce8-46a7-be11-0744a05200df-a354b42b.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:28 [logger.py:49] Received request chatcmpl-3f34a920-0ff5-48c6-9020-396b1f383e9d: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:28 [async_llm.py:382] Added request chatcmpl-3f34a920-0ff5-48c6-9020-396b1f383e9d-bde74fb1.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:28 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:29 [logger.py:49] Received request chatcmpl-4d9eae06-ae18-4648-b4bb-49e15e682283: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:29 [async_llm.py:382] Added request chatcmpl-4d9eae06-ae18-4648-b4bb-49e15e682283-b12024b3.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:29 [logger.py:49] Received request chatcmpl-b916b894-01ca-4c26-bee3-07b8f42535cb: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:29 [async_llm.py:382] Added request chatcmpl-b916b894-01ca-4c26-bee3-07b8f42535cb-8138b847.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:29 [logger.py:49] Received request chatcmpl-b9300726-a27a-4ff1-84c3-74e75122b879: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:29 [async_llm.py:382] Added request chatcmpl-b9300726-a27a-4ff1-84c3-74e75122b879-92f56386.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:29 [logger.py:49] Received request chatcmpl-8324303e-cb7b-4550-a221-c8066169dea6: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:29 [async_llm.py:382] Added request chatcmpl-8324303e-cb7b-4550-a221-c8066169dea6-aeb7bc24.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:29 [logger.py:49] Received request chatcmpl-7be17991-e17b-48d2-aa19-f9f44238a086: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:29 [async_llm.py:382] Added request chatcmpl-7be17991-e17b-48d2-aa19-f9f44238a086-a9231034.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [encoder_cache_manager.py:118] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:29 [logger.py:49] Received request chatcmpl-ea7d48f3-f249-4b43-8034-50c290dd1d62: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:29 [async_llm.py:382] Added request chatcmpl-ea7d48f3-f249-4b43-8034-50c290dd1d62-b7ddba99.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [encoder_cache_manager.py:118] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:29 [logger.py:49] Received request chatcmpl-9434c0e8-6006-41f8-8784-113d59136f43: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:29 [async_llm.py:382] Added request chatcmpl-9434c0e8-6006-41f8-8784-113d59136f43-b365cc0b.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:29 [logger.py:49] Received request chatcmpl-9451b099-4669-4c5b-9e1a-40ec7b30c07b: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:29 [async_llm.py:382] Added request chatcmpl-9451b099-4669-4c5b-9e1a-40ec7b30c07b-adf58e7d.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:29 [logger.py:49] Received request chatcmpl-1457439b-a575-48cc-b006-0bc360888a7b: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:29 [async_llm.py:382] Added request chatcmpl-1457439b-a575-48cc-b006-0bc360888a7b-8b20fec8.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:29 [logger.py:49] Received request chatcmpl-e096752b-8eae-4baf-b54b-f9ae5c789652: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:29 [async_llm.py:382] Added request chatcmpl-e096752b-8eae-4baf-b54b-f9ae5c789652-b61b4fa0.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:29 [logger.py:49] Received request chatcmpl-38296db8-568f-4a19-b60b-a7602d8bb22f: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:29 [async_llm.py:382] Added request chatcmpl-38296db8-568f-4a19-b60b-a7602d8bb22f-a5da173a.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [encoder_cache_manager.py:118] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:29 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:30 [logger.py:49] Received request chatcmpl-3ade2220-46a9-44e0-b6b1-7b1acbb7f2d9: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:30 [async_llm.py:382] Added request chatcmpl-3ade2220-46a9-44e0-b6b1-7b1acbb7f2d9-ab7ccaad.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:30 [logger.py:49] Received request chatcmpl-d5a37211-02a3-4d9c-8744-d9579f0cc428: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:30 [async_llm.py:382] Added request chatcmpl-d5a37211-02a3-4d9c-8744-d9579f0cc428-9b5d1296.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [encoder_cache_manager.py:118] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:30 [logger.py:49] Received request chatcmpl-6b0fd4d0-bdee-4d86-be62-1cb6455e2a11: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:30 [async_llm.py:382] Added request chatcmpl-6b0fd4d0-bdee-4d86-be62-1cb6455e2a11-b1a66837.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:30 [logger.py:49] Received request chatcmpl-6f7d8657-96af-4a98-95ab-01e045ddc4eb: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:30 [async_llm.py:382] Added request chatcmpl-6f7d8657-96af-4a98-95ab-01e045ddc4eb-8e507153.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:30 [logger.py:49] Received request chatcmpl-2acf73bb-9de3-4ccf-9793-22c5e28b9583: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:30 [async_llm.py:382] Added request chatcmpl-2acf73bb-9de3-4ccf-9793-22c5e28b9583-961dad6d.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:30 [logger.py:49] Received request chatcmpl-989b6f93-af8c-435c-9dd0-7eadb241f3d3: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:30 [async_llm.py:382] Added request chatcmpl-989b6f93-af8c-435c-9dd0-7eadb241f3d3-922a7b02.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:30 [logger.py:49] Received request chatcmpl-06568dea-e464-4100-9f6b-8a64cc0e4046: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:30 [async_llm.py:382] Added request chatcmpl-06568dea-e464-4100-9f6b-8a64cc0e4046-a91d16db.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [encoder_cache_manager.py:118] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:30 [logger.py:49] Received request chatcmpl-d8810b36-bc63-425b-ab17-a60e5d250773: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:30 [async_llm.py:382] Added request chatcmpl-d8810b36-bc63-425b-ab17-a60e5d250773-b792fa9d.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:30 [logger.py:49] Received request chatcmpl-37a32cdf-648b-4bb6-a494-879b91e665aa: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:30 [async_llm.py:382] Added request chatcmpl-37a32cdf-648b-4bb6-a494-879b91e665aa-985cd8a9.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:30 [logger.py:49] Received request chatcmpl-36be418d-4b40-4635-bde4-4ede452555ee: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:30 [async_llm.py:382] Added request chatcmpl-36be418d-4b40-4635-bde4-4ede452555ee-a8b58e82.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:30 [logger.py:49] Received request chatcmpl-d6939a6f-a44e-4444-893d-83dc2f2071df: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:30 [async_llm.py:382] Added request chatcmpl-d6939a6f-a44e-4444-893d-83dc2f2071df-a9a07340.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [encoder_cache_manager.py:118] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:30 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:31 [logger.py:49] Received request chatcmpl-086f1825-5e66-4d6c-91b0-e87c21425b3a: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:31 [async_llm.py:382] Added request chatcmpl-086f1825-5e66-4d6c-91b0-e87c21425b3a-b482044d.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:31 [logger.py:49] Received request chatcmpl-791350f1-e197-45d3-96f6-2979f743a1ee: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:31 [async_llm.py:382] Added request chatcmpl-791350f1-e197-45d3-96f6-2979f743a1ee-961923c1.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:31 [logger.py:49] Received request chatcmpl-546f9210-2e3b-41bb-91cd-992ea8faa940: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:31 [async_llm.py:382] Added request chatcmpl-546f9210-2e3b-41bb-91cd-992ea8faa940-952deeb0.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:31 [logger.py:49] Received request chatcmpl-58921780-f320-4a92-8095-a81e6597705a: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:31 [async_llm.py:382] Added request chatcmpl-58921780-f320-4a92-8095-a81e6597705a-bfc94bb9.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:31 [logger.py:49] Received request chatcmpl-adcde3f8-4f3e-4a92-9e50-53d06b03458a: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:31 [async_llm.py:382] Added request chatcmpl-adcde3f8-4f3e-4a92-9e50-53d06b03458a-bd6ecce0.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [encoder_cache_manager.py:118] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:31 [logger.py:49] Received request chatcmpl-cc376ba3-bbe9-47b4-aee6-99d26849d0cd: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:31 [async_llm.py:382] Added request chatcmpl-cc376ba3-bbe9-47b4-aee6-99d26849d0cd-ab3a02a3.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:31 [logger.py:49] Received request chatcmpl-fae1311b-0d0b-459f-abec-5e7f55a62635: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:31 [async_llm.py:382] Added request chatcmpl-fae1311b-0d0b-459f-abec-5e7f55a62635-9383a5f1.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:31 [logger.py:49] Received request chatcmpl-6fc4ee4b-aa63-49da-b07b-6bf13569c108: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:31 [async_llm.py:382] Added request chatcmpl-6fc4ee4b-aa63-49da-b07b-6bf13569c108-b23cc926.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [encoder_cache_manager.py:118] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:31 [logger.py:49] Received request chatcmpl-8f6b04a0-e30f-4283-910a-beb2267361f1: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:31 [async_llm.py:382] Added request chatcmpl-8f6b04a0-e30f-4283-910a-beb2267361f1-8cd410bf.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [encoder_cache_manager.py:118] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:31 [logger.py:49] Received request chatcmpl-64a5bec9-b610-4a2c-b0b3-859d73c3b318: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:31 [async_llm.py:382] Added request chatcmpl-64a5bec9-b610-4a2c-b0b3-859d73c3b318-98807355.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [encoder_cache_manager.py:118] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:31 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:32 [logger.py:49] Received request chatcmpl-37464c26-3005-4482-b11e-0968cfc2110f: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:32 [async_llm.py:382] Added request chatcmpl-37464c26-3005-4482-b11e-0968cfc2110f-a7181301.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [encoder_cache_manager.py:118] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:32 [logger.py:49] Received request chatcmpl-a6dba325-9ea4-48f2-a167-4488d724ba58: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:32 [async_llm.py:382] Added request chatcmpl-a6dba325-9ea4-48f2-a167-4488d724ba58-aa1dbab4.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:32 [logger.py:49] Received request chatcmpl-ce6f8813-0eeb-4ec6-8353-619fda7a108d: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:32 [async_llm.py:382] Added request chatcmpl-ce6f8813-0eeb-4ec6-8353-619fda7a108d-afef02c2.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [encoder_cache_manager.py:118] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:32 [logger.py:49] Received request chatcmpl-f63c9e8d-9b78-43c7-9e88-2d7ce61d60bb: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:32 [async_llm.py:382] Added request chatcmpl-f63c9e8d-9b78-43c7-9e88-2d7ce61d60bb-8b9bc1dc.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [encoder_cache_manager.py:118] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:32 [logger.py:49] Received request chatcmpl-22fe52b3-6df3-4bc2-9674-880202dde15f: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:32 [async_llm.py:382] Added request chatcmpl-22fe52b3-6df3-4bc2-9674-880202dde15f-8d35839a.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:32 [logger.py:49] Received request chatcmpl-6fb15e20-df28-4a0d-bece-fba214a0e1cc: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:32 [async_llm.py:382] Added request chatcmpl-6fb15e20-df28-4a0d-bece-fba214a0e1cc-95e654b6.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:32 [logger.py:49] Received request chatcmpl-2117f8e2-9148-4596-acce-609eb35cbed5: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:32 [async_llm.py:382] Added request chatcmpl-2117f8e2-9148-4596-acce-609eb35cbed5-9ec1f75d.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:32 [logger.py:49] Received request chatcmpl-0878038e-cf9c-45f8-96ee-c729a258f9d5: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:32 [async_llm.py:382] Added request chatcmpl-0878038e-cf9c-45f8-96ee-c729a258f9d5-ab68cc15.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:32 [logger.py:49] Received request chatcmpl-b1ed1962-6dae-443f-af49-f87927ff4fd5: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:32 [async_llm.py:382] Added request chatcmpl-b1ed1962-6dae-443f-af49-f87927ff4fd5-8fa4d06e.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:32 [logger.py:49] Received request chatcmpl-d05c26fd-02a8-477d-b7a8-bce01cd0d320: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:32 [async_llm.py:382] Added request chatcmpl-d05c26fd-02a8-477d-b7a8-bce01cd0d320-990fd462.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:32 [logger.py:49] Received request chatcmpl-d67a492d-0eaf-488e-a43b-59110634eed1: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:32 [async_llm.py:382] Added request chatcmpl-d67a492d-0eaf-488e-a43b-59110634eed1-aac23a9b.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:32 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:33 [logger.py:49] Received request chatcmpl-1f4d085b-7588-4d28-a88f-8b5e42badc7f: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:33 [async_llm.py:382] Added request chatcmpl-1f4d085b-7588-4d28-a88f-8b5e42badc7f-8da1ea88.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:33 [logger.py:49] Received request chatcmpl-05da325a-26ca-4c52-9008-29f830d08525: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:33 [async_llm.py:382] Added request chatcmpl-05da325a-26ca-4c52-9008-29f830d08525-905687c4.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:33 [logger.py:49] Received request chatcmpl-f18f0eb1-8417-4eb7-be7e-5a7b3429082a: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:33 [async_llm.py:382] Added request chatcmpl-f18f0eb1-8417-4eb7-be7e-5a7b3429082a-85c85a29.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:33 [logger.py:49] Received request chatcmpl-63474c08-c0a6-453c-85cc-29b906d3b155: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:33 [async_llm.py:382] Added request chatcmpl-63474c08-c0a6-453c-85cc-29b906d3b155-b798d8d2.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:33 [logger.py:49] Received request chatcmpl-1a6c1b57-5a11-4094-a37c-8ca30f240a7a: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:33 [async_llm.py:382] Added request chatcmpl-1a6c1b57-5a11-4094-a37c-8ca30f240a7a-bc09f3c0.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [encoder_cache_manager.py:118] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:33 [logger.py:49] Received request chatcmpl-664886d3-ea46-4555-af38-ea48befecd99: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:33 [async_llm.py:382] Added request chatcmpl-664886d3-ea46-4555-af38-ea48befecd99-be2e294b.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:33 [logger.py:49] Received request chatcmpl-abbc3d4e-80cb-46b6-9635-74da72ec0189: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:33 [async_llm.py:382] Added request chatcmpl-abbc3d4e-80cb-46b6-9635-74da72ec0189-9982d9aa.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:33 [logger.py:49] Received request chatcmpl-3e08678a-f03d-4bab-8cbd-6b0becd5ba2a: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:33 [async_llm.py:382] Added request chatcmpl-3e08678a-f03d-4bab-8cbd-6b0becd5ba2a-bf3303f5.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:33 [logger.py:49] Received request chatcmpl-f026cdd9-f54f-41b4-bea9-365273d11a34: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:33 [async_llm.py:382] Added request chatcmpl-f026cdd9-f54f-41b4-bea9-365273d11a34-a2ed6c30.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [encoder_cache_manager.py:118] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:33 [logger.py:49] Received request chatcmpl-3db0ef4c-3a2c-42d2-9126-99068ab41f93: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:33 [async_llm.py:382] Added request chatcmpl-3db0ef4c-3a2c-42d2-9126-99068ab41f93-b447f42d.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:33 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:34 [logger.py:49] Received request chatcmpl-88646670-6b81-4041-8c24-28f2a8eef6f5: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:34 [async_llm.py:382] Added request chatcmpl-88646670-6b81-4041-8c24-28f2a8eef6f5-847a5329.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:34 [logger.py:49] Received request chatcmpl-d713750a-222e-4cf3-9acd-acffca6287a1: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:34 [async_llm.py:382] Added request chatcmpl-d713750a-222e-4cf3-9acd-acffca6287a1-90af28f7.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:34 [logger.py:49] Received request chatcmpl-d14c74d2-4486-4bb5-ba68-af7d2d667754: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:34 [async_llm.py:382] Added request chatcmpl-d14c74d2-4486-4bb5-ba68-af7d2d667754-bea9f88e.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:34 [logger.py:49] Received request chatcmpl-39f70a1f-ba28-4fa3-ab83-1fc5ce23b174: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:34 [async_llm.py:382] Added request chatcmpl-39f70a1f-ba28-4fa3-ab83-1fc5ce23b174-86f6a340.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:34 [logger.py:49] Received request chatcmpl-7358c5e6-15e3-4169-96a2-ffab1f6e723f: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:34 [async_llm.py:382] Added request chatcmpl-7358c5e6-15e3-4169-96a2-ffab1f6e723f-9ec92dd4.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [encoder_cache_manager.py:118] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:34 [logger.py:49] Received request chatcmpl-8346a92b-8a6b-41d9-8ff4-3ad223d8015f: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:34 [async_llm.py:382] Added request chatcmpl-8346a92b-8a6b-41d9-8ff4-3ad223d8015f-b0be3612.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [encoder_cache_manager.py:118] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:34 [logger.py:49] Received request chatcmpl-f2295d92-5061-4d01-901b-40649b87b3a4: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:34 [async_llm.py:382] Added request chatcmpl-f2295d92-5061-4d01-901b-40649b87b3a4-9ef4d669.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:34 [logger.py:49] Received request chatcmpl-b1a19d65-3f83-4cf3-8911-3c72e3729657: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:34 [async_llm.py:382] Added request chatcmpl-b1a19d65-3f83-4cf3-8911-3c72e3729657-a358ff24.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [encoder_cache_manager.py:118] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:34 [logger.py:49] Received request chatcmpl-753be4ff-39cf-4bda-9789-160ef79038a5: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:34 [async_llm.py:382] Added request chatcmpl-753be4ff-39cf-4bda-9789-160ef79038a5-8197f558.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:34 [logger.py:49] Received request chatcmpl-0effcff6-9e02-4910-9b7e-1f1a0f8d4584: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:34 [async_llm.py:382] Added request chatcmpl-0effcff6-9e02-4910-9b7e-1f1a0f8d4584-8c5af53a.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:34 [logger.py:49] Received request chatcmpl-1657bf83-12dd-434a-a56c-b42ab9110081: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:34 [async_llm.py:382] Added request chatcmpl-1657bf83-12dd-434a-a56c-b42ab9110081-a2d018ea.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:34 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:35 [logger.py:49] Received request chatcmpl-27b0db34-ac39-4336-9659-0371357146c7: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:35 [async_llm.py:382] Added request chatcmpl-27b0db34-ac39-4336-9659-0371357146c7-88a7ce7c.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:35 [logger.py:49] Received request chatcmpl-e7da27be-a6e9-4f1c-883a-846f1e595d64: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:35 [async_llm.py:382] Added request chatcmpl-e7da27be-a6e9-4f1c-883a-846f1e595d64-811d536f.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:35 [logger.py:49] Received request chatcmpl-875fc860-b4b1-4397-b023-d34b97815250: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:35 [async_llm.py:382] Added request chatcmpl-875fc860-b4b1-4397-b023-d34b97815250-92c70951.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [encoder_cache_manager.py:118] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:35 [logger.py:49] Received request chatcmpl-85432ede-ebab-4e22-80ff-76736306c7c5: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:35 [async_llm.py:382] Added request chatcmpl-85432ede-ebab-4e22-80ff-76736306c7c5-9a2f5465.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:35 [logger.py:49] Received request chatcmpl-9579f147-a570-40af-9d51-8edd66492d49: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:35 [async_llm.py:382] Added request chatcmpl-9579f147-a570-40af-9d51-8edd66492d49-913a00a9.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [encoder_cache_manager.py:118] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:35 [logger.py:49] Received request chatcmpl-a8cfa06a-aabc-4a05-aac1-df575e65082b: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:35 [async_llm.py:382] Added request chatcmpl-a8cfa06a-aabc-4a05-aac1-df575e65082b-a582734b.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:35 [logger.py:49] Received request chatcmpl-3995b829-abd8-427a-a176-590ed1f256f0: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:35 [async_llm.py:382] Added request chatcmpl-3995b829-abd8-427a-a176-590ed1f256f0-a8e54f49.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:35 [logger.py:49] Received request chatcmpl-e84e998d-3cc9-4556-82be-66ca80b7c110: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:35 [async_llm.py:382] Added request chatcmpl-e84e998d-3cc9-4556-82be-66ca80b7c110-bfa4382f.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [encoder_cache_manager.py:118] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:35 [logger.py:49] Received request chatcmpl-908ed228-2c9f-4cda-ad6f-59ee43c93bff: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:35 [async_llm.py:382] Added request chatcmpl-908ed228-2c9f-4cda-ad6f-59ee43c93bff-bf819edd.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:35 [logger.py:49] Received request chatcmpl-032117c1-d418-4f55-ba21-63c56f8c31b8: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:35 [async_llm.py:382] Added request chatcmpl-032117c1-d418-4f55-ba21-63c56f8c31b8-8e71fabc.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:35 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:35 [logger.py:49] Received request chatcmpl-ceb176a8-2cf5-44c6-b674-bdac10ad5918: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:36 [async_llm.py:382] Added request chatcmpl-ceb176a8-2cf5-44c6-b674-bdac10ad5918-b3aa1645.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:36 [logger.py:49] Received request chatcmpl-064cf361-18ac-46b3-936a-c4bdd22c2178: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:36 [async_llm.py:382] Added request chatcmpl-064cf361-18ac-46b3-936a-c4bdd22c2178-86123665.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [encoder_cache_manager.py:118] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:36 [logger.py:49] Received request chatcmpl-71c93ce5-21cc-40f5-a895-e2fde8e9b682: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:36 [async_llm.py:382] Added request chatcmpl-71c93ce5-21cc-40f5-a895-e2fde8e9b682-937333c9.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [encoder_cache_manager.py:118] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:36 [logger.py:49] Received request chatcmpl-46a7705c-9644-428a-8301-bb27193ddff2: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:36 [async_llm.py:382] Added request chatcmpl-46a7705c-9644-428a-8301-bb27193ddff2-b092c2df.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:36 [logger.py:49] Received request chatcmpl-9f1e0392-e261-47d5-b8ab-432e6ac82963: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:36 [async_llm.py:382] Added request chatcmpl-9f1e0392-e261-47d5-b8ab-432e6ac82963-9cf300cd.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:36 [logger.py:49] Received request chatcmpl-43aec3a5-c698-414e-8666-0aa35f13f116: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:36 [async_llm.py:382] Added request chatcmpl-43aec3a5-c698-414e-8666-0aa35f13f116-a8b5a2d8.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [encoder_cache_manager.py:118] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:36 [logger.py:49] Received request chatcmpl-bb4e5312-3fa2-4648-92b3-2738e7f66412: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:36 [async_llm.py:382] Added request chatcmpl-bb4e5312-3fa2-4648-92b3-2738e7f66412-abc9ac9f.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [encoder_cache_manager.py:118] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:36 [logger.py:49] Received request chatcmpl-04935ea1-fb01-4588-9f1b-d6a1eebf78dd: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:36 [async_llm.py:382] Added request chatcmpl-04935ea1-fb01-4588-9f1b-d6a1eebf78dd-a1162bb3.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [encoder_cache_manager.py:118] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:36 [logger.py:49] Received request chatcmpl-6918bf9b-27e2-44f6-af17-933c84750d10: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:36 [async_llm.py:382] Added request chatcmpl-6918bf9b-27e2-44f6-af17-933c84750d10-b2d9c75c.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:36 [logger.py:49] Received request chatcmpl-c704fff8-a705-4056-a7e6-ca7b23e603a3: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:36 [async_llm.py:382] Added request chatcmpl-c704fff8-a705-4056-a7e6-ca7b23e603a3-a22d922a.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:36 [logger.py:49] Received request chatcmpl-f19dc290-32fa-4703-a29e-59ac6a58b827: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:36 [async_llm.py:382] Added request chatcmpl-f19dc290-32fa-4703-a29e-59ac6a58b827-9b4ab0a7.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [encoder_cache_manager.py:118] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:36 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:37 [logger.py:49] Received request chatcmpl-094e2cdc-886e-4da2-a15c-758e8e653388: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:37 [async_llm.py:382] Added request chatcmpl-094e2cdc-886e-4da2-a15c-758e8e653388-84d542b6.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:37 [logger.py:49] Received request chatcmpl-9ffafa00-9923-46b9-9fd8-d19b4f52b536: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:37 [async_llm.py:382] Added request chatcmpl-9ffafa00-9923-46b9-9fd8-d19b4f52b536-bd75d1df.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:37 [logger.py:49] Received request chatcmpl-2b1c895e-2e57-465a-91d7-d02689da8b38: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:37 [async_llm.py:382] Added request chatcmpl-2b1c895e-2e57-465a-91d7-d02689da8b38-baf9b67a.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:37 [logger.py:49] Received request chatcmpl-f6d441da-f875-49e8-b6bf-4f1d5cdd604c: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:37 [async_llm.py:382] Added request chatcmpl-f6d441da-f875-49e8-b6bf-4f1d5cdd604c-94f2a538.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:37 [logger.py:49] Received request chatcmpl-9e772d80-359d-45a1-8045-24203ae1b773: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:37 [async_llm.py:382] Added request chatcmpl-9e772d80-359d-45a1-8045-24203ae1b773-b807a720.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [encoder_cache_manager.py:118] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:37 [loggers.py:263] Engine 000: Avg prompt throughput: 5886.2 tokens/s, Avg generation throughput: 10.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 96.2%, Encoder cache hit rate: 96.2%
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:37 [logger.py:49] Received request chatcmpl-fa2f691d-cb85-47d5-9245-15a5995f5074: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:37 [async_llm.py:382] Added request chatcmpl-fa2f691d-cb85-47d5-9245-15a5995f5074-b78e4186.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [encoder_cache_manager.py:118] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:37 [logger.py:49] Received request chatcmpl-23c3fd37-7983-4ba6-873d-6b55c55ee9ea: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:37 [async_llm.py:382] Added request chatcmpl-23c3fd37-7983-4ba6-873d-6b55c55ee9ea-80dad4d3.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [encoder_cache_manager.py:118] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:37 [logger.py:49] Received request chatcmpl-b7d9ff7f-b09d-4e63-a579-4e6d99976dbe: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:37 [async_llm.py:382] Added request chatcmpl-b7d9ff7f-b09d-4e63-a579-4e6d99976dbe-a5685093.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:37 [logger.py:49] Received request chatcmpl-8b9bbcd0-3f79-4a2a-a2a1-1767c8c8c8ef: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:37 [async_llm.py:382] Added request chatcmpl-8b9bbcd0-3f79-4a2a-a2a1-1767c8c8c8ef-9c5ed822.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [encoder_cache_manager.py:118] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:37 [logger.py:49] Received request chatcmpl-aaf6ea8b-a134-4867-b581-e6037e084fc4: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:37 [async_llm.py:382] Added request chatcmpl-aaf6ea8b-a134-4867-b581-e6037e084fc4-87895090.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:37 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:38 [logger.py:49] Received request chatcmpl-b4fd86ef-2fbf-4358-b9ac-fb4a9aada2a7: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:38 [async_llm.py:382] Added request chatcmpl-b4fd86ef-2fbf-4358-b9ac-fb4a9aada2a7-a7df7778.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:38 [logger.py:49] Received request chatcmpl-96e78ddb-d463-45f4-a7d1-d850b7b1a681: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:38 [async_llm.py:382] Added request chatcmpl-96e78ddb-d463-45f4-a7d1-d850b7b1a681-9a3dfcd9.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:38 [logger.py:49] Received request chatcmpl-76b0e232-f925-4839-b124-3fc352ec9089: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:38 [async_llm.py:382] Added request chatcmpl-76b0e232-f925-4839-b124-3fc352ec9089-883fbda8.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:38 [logger.py:49] Received request chatcmpl-a9f0d093-f5c0-49bd-86ed-208807d25250: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:38 [async_llm.py:382] Added request chatcmpl-a9f0d093-f5c0-49bd-86ed-208807d25250-87e6d30f.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:38 [logger.py:49] Received request chatcmpl-e09f41a9-3de8-487e-b2f6-f1ae9480fc1f: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:38 [async_llm.py:382] Added request chatcmpl-e09f41a9-3de8-487e-b2f6-f1ae9480fc1f-a14ad368.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:38 [logger.py:49] Received request chatcmpl-ade6d837-d102-4e7f-8eda-5574e893a513: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:38 [async_llm.py:382] Added request chatcmpl-ade6d837-d102-4e7f-8eda-5574e893a513-a949fe06.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [encoder_cache_manager.py:118] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:38 [logger.py:49] Received request chatcmpl-54bb94bf-4c70-4d93-89e5-b9fc3f1a80b5: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:38 [async_llm.py:382] Added request chatcmpl-54bb94bf-4c70-4d93-89e5-b9fc3f1a80b5-8cfce443.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:38 [logger.py:49] Received request chatcmpl-59e6e2b9-a66e-4d07-8ec5-27babb7ee309: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:38 [async_llm.py:382] Added request chatcmpl-59e6e2b9-a66e-4d07-8ec5-27babb7ee309-a6a575f4.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:38 [logger.py:49] Received request chatcmpl-5bf1af86-87a6-4bdd-b753-a544b5e7b6e0: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:38 [async_llm.py:382] Added request chatcmpl-5bf1af86-87a6-4bdd-b753-a544b5e7b6e0-9cb69a29.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:38 [logger.py:49] Received request chatcmpl-d9fc2260-ebeb-4190-aa95-32ddafdafbc6: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:38 [async_llm.py:382] Added request chatcmpl-d9fc2260-ebeb-4190-aa95-32ddafdafbc6-a188f972.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:38 [logger.py:49] Received request chatcmpl-02a8e71c-e091-4547-9e39-06f8d958ff4f: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:38 [async_llm.py:382] Added request chatcmpl-02a8e71c-e091-4547-9e39-06f8d958ff4f-a5b66c95.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:38 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:39 [logger.py:49] Received request chatcmpl-528ad030-73f0-4f51-9115-3bd271c9b6ab: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:39 [async_llm.py:382] Added request chatcmpl-528ad030-73f0-4f51-9115-3bd271c9b6ab-84ac271f.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:39 [logger.py:49] Received request chatcmpl-609ff53f-8b73-4fe1-a851-1d45cb025e64: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:39 [async_llm.py:382] Added request chatcmpl-609ff53f-8b73-4fe1-a851-1d45cb025e64-adf36a72.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [encoder_cache_manager.py:118] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:39 [logger.py:49] Received request chatcmpl-7e3f2d9d-9973-44be-93e0-0ff0c889341e: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:39 [async_llm.py:382] Added request chatcmpl-7e3f2d9d-9973-44be-93e0-0ff0c889341e-b6afb92a.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:39 [logger.py:49] Received request chatcmpl-c1dbcb5e-53a2-4fd6-a4ce-287aee796439: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:39 [async_llm.py:382] Added request chatcmpl-c1dbcb5e-53a2-4fd6-a4ce-287aee796439-a2664946.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:39 [logger.py:49] Received request chatcmpl-d8540fa5-21d0-4129-800b-3cc0a9ad998a: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:39 [async_llm.py:382] Added request chatcmpl-d8540fa5-21d0-4129-800b-3cc0a9ad998a-80b2c2cf.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [encoder_cache_manager.py:118] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:39 [logger.py:49] Received request chatcmpl-39fd4635-bee0-43d1-b588-4b3318a4c902: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:39 [async_llm.py:382] Added request chatcmpl-39fd4635-bee0-43d1-b588-4b3318a4c902-b9a07b57.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:39 [logger.py:49] Received request chatcmpl-2a3270cd-d58d-4239-9d64-ff15c52dd865: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:39 [async_llm.py:382] Added request chatcmpl-2a3270cd-d58d-4239-9d64-ff15c52dd865-9b6e0a60.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [encoder_cache_manager.py:118] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:39 [logger.py:49] Received request chatcmpl-9087e8fe-2a28-4150-9eab-7a6732a3d27d: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:39 [async_llm.py:382] Added request chatcmpl-9087e8fe-2a28-4150-9eab-7a6732a3d27d-95a15cb6.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:39 [logger.py:49] Received request chatcmpl-ec27220d-a1fb-4dbf-a498-ce073d0f67b7: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:39 [async_llm.py:382] Added request chatcmpl-ec27220d-a1fb-4dbf-a498-ce073d0f67b7-9d06b18c.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:39 [logger.py:49] Received request chatcmpl-cb926ba4-d575-4a8e-aa7a-b53cce3b50ac: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:39 [async_llm.py:382] Added request chatcmpl-cb926ba4-d575-4a8e-aa7a-b53cce3b50ac-bd910e47.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:39 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:40 [logger.py:49] Received request chatcmpl-fe0429d9-e864-4ddc-8c72-9bec9cf5d4d2: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:40 [async_llm.py:382] Added request chatcmpl-fe0429d9-e864-4ddc-8c72-9bec9cf5d4d2-9df9e57b.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:40 [logger.py:49] Received request chatcmpl-12c6ead5-d030-406e-9ed8-c8e871a41e37: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:40 [async_llm.py:382] Added request chatcmpl-12c6ead5-d030-406e-9ed8-c8e871a41e37-83696308.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:40 [logger.py:49] Received request chatcmpl-01754a25-7763-40ba-b2e6-e341720f801f: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:40 [async_llm.py:382] Added request chatcmpl-01754a25-7763-40ba-b2e6-e341720f801f-ae554f7a.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [encoder_cache_manager.py:118] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:40 [logger.py:49] Received request chatcmpl-27b6df07-4613-4661-9f60-80a22cfcd95c: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:40 [async_llm.py:382] Added request chatcmpl-27b6df07-4613-4661-9f60-80a22cfcd95c-a06e6186.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:40 [logger.py:49] Received request chatcmpl-2bddf650-93fb-40b1-9d46-655a8e84114d: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:40 [async_llm.py:382] Added request chatcmpl-2bddf650-93fb-40b1-9d46-655a8e84114d-b4b8c5e0.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:40 [logger.py:49] Received request chatcmpl-3124b072-c10d-42ab-8bbd-07bd98cd70a0: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:40 [async_llm.py:382] Added request chatcmpl-3124b072-c10d-42ab-8bbd-07bd98cd70a0-96f8d713.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:40 [logger.py:49] Received request chatcmpl-2c82d5ba-469a-4f88-a77e-7fa9d3d31edf: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:40 [async_llm.py:382] Added request chatcmpl-2c82d5ba-469a-4f88-a77e-7fa9d3d31edf-987b16c1.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:40 [logger.py:49] Received request chatcmpl-94e0ab4e-8cbd-409b-b8a8-2f9e4b49fc95: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:40 [async_llm.py:382] Added request chatcmpl-94e0ab4e-8cbd-409b-b8a8-2f9e4b49fc95-987f8c09.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:40 [logger.py:49] Received request chatcmpl-79c61237-e722-4087-a6ef-73a454b746c8: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:40 [async_llm.py:382] Added request chatcmpl-79c61237-e722-4087-a6ef-73a454b746c8-b9a1531f.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:40 [logger.py:49] Received request chatcmpl-184aee3e-ec24-4e89-b710-6f5e530ed034: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:40 [async_llm.py:382] Added request chatcmpl-184aee3e-ec24-4e89-b710-6f5e530ed034-bc109cb7.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [encoder_cache_manager.py:118] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:40 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:41 [logger.py:49] Received request chatcmpl-14708d89-f249-48ba-b133-fbd0df1cfda9: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:41 [async_llm.py:382] Added request chatcmpl-14708d89-f249-48ba-b133-fbd0df1cfda9-a3197baa.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [encoder_cache_manager.py:118] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:41 [logger.py:49] Received request chatcmpl-f85f496a-7b71-4069-9586-88a4cb5516a3: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:41 [async_llm.py:382] Added request chatcmpl-f85f496a-7b71-4069-9586-88a4cb5516a3-8b7f27d6.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [encoder_cache_manager.py:118] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:41 [logger.py:49] Received request chatcmpl-2d834d63-5f58-44fa-b134-70b04d7b0ff6: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:41 [async_llm.py:382] Added request chatcmpl-2d834d63-5f58-44fa-b134-70b04d7b0ff6-a03507a9.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:41 [logger.py:49] Received request chatcmpl-8d5bfc34-97c7-488a-b557-ec9e5724443c: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:41 [async_llm.py:382] Added request chatcmpl-8d5bfc34-97c7-488a-b557-ec9e5724443c-9cb06b50.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:41 [logger.py:49] Received request chatcmpl-df1669b7-c703-4e20-91c7-05900af3a4e5: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:41 [async_llm.py:382] Added request chatcmpl-df1669b7-c703-4e20-91c7-05900af3a4e5-845989be.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:41 [logger.py:49] Received request chatcmpl-091334f4-8f82-4be8-ad9a-076d424964fc: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:41 [async_llm.py:382] Added request chatcmpl-091334f4-8f82-4be8-ad9a-076d424964fc-a57c6e41.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [encoder_cache_manager.py:118] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:41 [logger.py:49] Received request chatcmpl-306eb43e-373e-4bad-add3-e8fe24e1d929: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:41 [async_llm.py:382] Added request chatcmpl-306eb43e-373e-4bad-add3-e8fe24e1d929-a8da6f9f.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:41 [logger.py:49] Received request chatcmpl-7e6bcd44-36a0-46d2-b7ef-937bff73c5b1: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:41 [async_llm.py:382] Added request chatcmpl-7e6bcd44-36a0-46d2-b7ef-937bff73c5b1-a3ca6810.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:41 [logger.py:49] Received request chatcmpl-d9f23873-e3ad-4a52-bcdd-59e804432e0f: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:41 [async_llm.py:382] Added request chatcmpl-d9f23873-e3ad-4a52-bcdd-59e804432e0f-88aa3e7e.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:41 [logger.py:49] Received request chatcmpl-e7af76ed-a0a8-4ea3-998c-16ed3bb13b3f: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:41 [async_llm.py:382] Added request chatcmpl-e7af76ed-a0a8-4ea3-998c-16ed3bb13b3f-b27d917f.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:41 [logger.py:49] Received request chatcmpl-c8973d51-a9ff-4632-89f9-4bc71c877d79: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:41 [async_llm.py:382] Added request chatcmpl-c8973d51-a9ff-4632-89f9-4bc71c877d79-98ab60a5.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:41 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:42 [logger.py:49] Received request chatcmpl-f6d442d9-1512-4b9b-ab62-55c862f779f9: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:42 [async_llm.py:382] Added request chatcmpl-f6d442d9-1512-4b9b-ab62-55c862f779f9-a5a20192.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [encoder_cache_manager.py:118] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:42 [logger.py:49] Received request chatcmpl-614702d1-5d6c-418e-9a67-858c7a9d7bed: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:42 [async_llm.py:382] Added request chatcmpl-614702d1-5d6c-418e-9a67-858c7a9d7bed-b9273a0e.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:42 [logger.py:49] Received request chatcmpl-0e760e50-7d02-4e18-b324-9a9fd7292640: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:42 [async_llm.py:382] Added request chatcmpl-0e760e50-7d02-4e18-b324-9a9fd7292640-a0877424.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [encoder_cache_manager.py:118] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:42 [logger.py:49] Received request chatcmpl-0bc4ee88-242b-43e8-b404-5a92a0dfabf3: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:42 [async_llm.py:382] Added request chatcmpl-0bc4ee88-242b-43e8-b404-5a92a0dfabf3-a8dda966.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:42 [logger.py:49] Received request chatcmpl-d03b2678-13ee-49dc-aabd-e30a378dc0a2: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:42 [async_llm.py:382] Added request chatcmpl-d03b2678-13ee-49dc-aabd-e30a378dc0a2-a9e1844c.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:42 [logger.py:49] Received request chatcmpl-58c810fd-58bb-4b34-be6a-ef19355bc0a8: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:42 [async_llm.py:382] Added request chatcmpl-58c810fd-58bb-4b34-be6a-ef19355bc0a8-84897843.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:42 [logger.py:49] Received request chatcmpl-1d49953a-d7e1-46fa-af8e-f7f08a43e669: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:42 [async_llm.py:382] Added request chatcmpl-1d49953a-d7e1-46fa-af8e-f7f08a43e669-ae688673.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:42 [logger.py:49] Received request chatcmpl-56371e4c-2d39-488a-a484-32261312e659: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:42 [async_llm.py:382] Added request chatcmpl-56371e4c-2d39-488a-a484-32261312e659-90736471.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:42 [logger.py:49] Received request chatcmpl-05091e98-2b15-476a-99c6-a6936a7b9985: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:42 [async_llm.py:382] Added request chatcmpl-05091e98-2b15-476a-99c6-a6936a7b9985-b8fda2f9.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:42 [logger.py:49] Received request chatcmpl-8f518889-927a-4e43-8147-dd046ae714d4: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:42 [async_llm.py:382] Added request chatcmpl-8f518889-927a-4e43-8147-dd046ae714d4-aa78124b.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [encoder_cache_manager.py:118] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:42 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:43 [logger.py:49] Received request chatcmpl-31edc4f7-46dc-4245-a651-58003dc97630: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:43 [async_llm.py:382] Added request chatcmpl-31edc4f7-46dc-4245-a651-58003dc97630-9b1592f7.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:43 [logger.py:49] Received request chatcmpl-0ce1d19e-e551-4213-9bcd-6dc32432d0f7: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:43 [async_llm.py:382] Added request chatcmpl-0ce1d19e-e551-4213-9bcd-6dc32432d0f7-a3ca67f2.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:43 [logger.py:49] Received request chatcmpl-ee91e0ba-56dd-4447-a0d8-b2303ea09728: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:43 [async_llm.py:382] Added request chatcmpl-ee91e0ba-56dd-4447-a0d8-b2303ea09728-9d276c42.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:43 [logger.py:49] Received request chatcmpl-42fef81a-b21e-430f-9e93-548552ceeda6: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:43 [async_llm.py:382] Added request chatcmpl-42fef81a-b21e-430f-9e93-548552ceeda6-aa2335da.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:43 [logger.py:49] Received request chatcmpl-fe6cd08c-7b04-4bbb-813f-d03adf8fbe73: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:43 [async_llm.py:382] Added request chatcmpl-fe6cd08c-7b04-4bbb-813f-d03adf8fbe73-8047a701.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [encoder_cache_manager.py:118] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:43 [logger.py:49] Received request chatcmpl-735f7fd2-7d29-4048-89b8-b1ad12526359: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:43 [async_llm.py:382] Added request chatcmpl-735f7fd2-7d29-4048-89b8-b1ad12526359-8eedb2aa.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:43 [logger.py:49] Received request chatcmpl-b18bf1a2-6fbf-4076-9b94-859daaa4e801: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:43 [async_llm.py:382] Added request chatcmpl-b18bf1a2-6fbf-4076-9b94-859daaa4e801-99728912.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [encoder_cache_manager.py:118] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:43 [logger.py:49] Received request chatcmpl-444a30ba-4109-4576-bedb-fa82b9c4bb44: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:43 [async_llm.py:382] Added request chatcmpl-444a30ba-4109-4576-bedb-fa82b9c4bb44-83a39221.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [encoder_cache_manager.py:118] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:43 [logger.py:49] Received request chatcmpl-60bd4159-79db-45f5-8c5a-88f645e33846: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:43 [async_llm.py:382] Added request chatcmpl-60bd4159-79db-45f5-8c5a-88f645e33846-a58e714f.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:43 [logger.py:49] Received request chatcmpl-be8af1d8-5344-4ceb-bdcf-102384359dbf: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:43 [async_llm.py:382] Added request chatcmpl-be8af1d8-5344-4ceb-bdcf-102384359dbf-84329ec1.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:43 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:44 [logger.py:49] Received request chatcmpl-cb02c0e2-1c80-402c-88c1-5cc7ec8aaee4: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:44 [async_llm.py:382] Added request chatcmpl-cb02c0e2-1c80-402c-88c1-5cc7ec8aaee4-9dae8189.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [encoder_cache_manager.py:118] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:44 [logger.py:49] Received request chatcmpl-bd94db8a-e195-499b-9013-c03961729863: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:44 [async_llm.py:382] Added request chatcmpl-bd94db8a-e195-499b-9013-c03961729863-a0941ae4.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:44 [logger.py:49] Received request chatcmpl-01d195ac-3494-4716-90a9-901be887c71d: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:44 [async_llm.py:382] Added request chatcmpl-01d195ac-3494-4716-90a9-901be887c71d-aff163f3.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [encoder_cache_manager.py:118] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:44 [logger.py:49] Received request chatcmpl-789bbbd4-e41b-4314-8f15-7b8a0b009349: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:44 [async_llm.py:382] Added request chatcmpl-789bbbd4-e41b-4314-8f15-7b8a0b009349-99a88aa7.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:44 [logger.py:49] Received request chatcmpl-b930cdfa-001b-43f0-872c-3a150719d1c8: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:44 [async_llm.py:382] Added request chatcmpl-b930cdfa-001b-43f0-872c-3a150719d1c8-945df232.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:44 [logger.py:49] Received request chatcmpl-5cea7b4c-def4-470a-94b6-03a2933d2478: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:44 [async_llm.py:382] Added request chatcmpl-5cea7b4c-def4-470a-94b6-03a2933d2478-82190587.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:44 [logger.py:49] Received request chatcmpl-474b8f4a-2392-4da3-939d-54b1bea8ba09: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:44 [async_llm.py:382] Added request chatcmpl-474b8f4a-2392-4da3-939d-54b1bea8ba09-95ef5d48.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:44 [logger.py:49] Received request chatcmpl-c8580c90-2ed6-4deb-a54e-8985c00b3607: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:44 [async_llm.py:382] Added request chatcmpl-c8580c90-2ed6-4deb-a54e-8985c00b3607-a554e1f5.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:44 [logger.py:49] Received request chatcmpl-e23bdb8d-fda2-48e7-9c14-401c21149b7b: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:44 [async_llm.py:382] Added request chatcmpl-e23bdb8d-fda2-48e7-9c14-401c21149b7b-8c287a2f.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:44 [logger.py:49] Received request chatcmpl-f5f99fae-4fde-436f-a45f-d3228bdcb906: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:44 [async_llm.py:382] Added request chatcmpl-f5f99fae-4fde-436f-a45f-d3228bdcb906-acd33320.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [encoder_cache_manager.py:118] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:44 [logger.py:49] Received request chatcmpl-a31bb1e8-23ce-4a23-a8f1-b7a34586075e: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:44 [async_llm.py:382] Added request chatcmpl-a31bb1e8-23ce-4a23-a8f1-b7a34586075e-a8a1c6f2.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:44 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:45 [logger.py:49] Received request chatcmpl-c1aefdb5-5615-4750-9e83-60f1bbc0960b: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:45 [async_llm.py:382] Added request chatcmpl-c1aefdb5-5615-4750-9e83-60f1bbc0960b-bb4cb775.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:45 [logger.py:49] Received request chatcmpl-b0ed22f3-1614-4aa9-b5b9-0d67ef9676d9: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:45 [async_llm.py:382] Added request chatcmpl-b0ed22f3-1614-4aa9-b5b9-0d67ef9676d9-b6b3cdab.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:45 [logger.py:49] Received request chatcmpl-a06d935e-6437-433c-a4fb-59a85b5ee170: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:45 [async_llm.py:382] Added request chatcmpl-a06d935e-6437-433c-a4fb-59a85b5ee170-920ce24b.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:45 [logger.py:49] Received request chatcmpl-6189bbfc-e35e-48ff-b46e-c96cf19fa2dc: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:45 [async_llm.py:382] Added request chatcmpl-6189bbfc-e35e-48ff-b46e-c96cf19fa2dc-a20d117a.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:45 [logger.py:49] Received request chatcmpl-6262cf58-a478-4aa1-864c-eee919891b54: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:45 [async_llm.py:382] Added request chatcmpl-6262cf58-a478-4aa1-864c-eee919891b54-bd041792.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:45 [logger.py:49] Received request chatcmpl-f76a8740-2170-4583-8538-53dcaa293fff: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:45 [async_llm.py:382] Added request chatcmpl-f76a8740-2170-4583-8538-53dcaa293fff-9c423536.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [encoder_cache_manager.py:118] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:45 [logger.py:49] Received request chatcmpl-12a64c15-cd25-4d0f-b456-11ae7ed35ec1: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:45 [async_llm.py:382] Added request chatcmpl-12a64c15-cd25-4d0f-b456-11ae7ed35ec1-aa7753d9.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:45 [logger.py:49] Received request chatcmpl-aab95fcd-34e7-4021-bb1e-fee466d2a517: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:45 [async_llm.py:382] Added request chatcmpl-aab95fcd-34e7-4021-bb1e-fee466d2a517-bb19baca.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [encoder_cache_manager.py:118] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:45 [logger.py:49] Received request chatcmpl-e818e209-a296-4930-8455-7b955b8c2385: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:45 [async_llm.py:382] Added request chatcmpl-e818e209-a296-4930-8455-7b955b8c2385-85441ba9.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:45 [logger.py:49] Received request chatcmpl-400480ca-344b-45ad-aabe-fc3163cddda7: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:45 [async_llm.py:382] Added request chatcmpl-400480ca-344b-45ad-aabe-fc3163cddda7-a2a0db01.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:45 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:46 [logger.py:49] Received request chatcmpl-8449be52-a23d-4a46-8319-a743c5b8a672: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:46 [async_llm.py:382] Added request chatcmpl-8449be52-a23d-4a46-8319-a743c5b8a672-87e63344.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:46 [logger.py:49] Received request chatcmpl-f1611ecf-6e74-4ba9-a583-0f404a1aacf9: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:46 [async_llm.py:382] Added request chatcmpl-f1611ecf-6e74-4ba9-a583-0f404a1aacf9-b3b1be6d.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:46 [logger.py:49] Received request chatcmpl-8cafff42-d7e6-4fde-a154-4bd987e474e8: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:46 [async_llm.py:382] Added request chatcmpl-8cafff42-d7e6-4fde-a154-4bd987e474e8-9c49f17a.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:46 [logger.py:49] Received request chatcmpl-22c88764-e742-47e9-9673-2bea8ccb683a: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:46 [async_llm.py:382] Added request chatcmpl-22c88764-e742-47e9-9673-2bea8ccb683a-ba092cf6.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [encoder_cache_manager.py:118] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:46 [logger.py:49] Received request chatcmpl-e1589d80-5078-4770-8325-628232ecbdfa: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:46 [async_llm.py:382] Added request chatcmpl-e1589d80-5078-4770-8325-628232ecbdfa-ab1f95c2.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:46 [logger.py:49] Received request chatcmpl-5e5c7411-b97c-4744-a16e-778936ff115c: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:46 [async_llm.py:382] Added request chatcmpl-5e5c7411-b97c-4744-a16e-778936ff115c-8bac0867.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:46 [logger.py:49] Received request chatcmpl-84437c7e-e469-40b0-87df-beca2446c982: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:46 [async_llm.py:382] Added request chatcmpl-84437c7e-e469-40b0-87df-beca2446c982-bd3aad74.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:46 [logger.py:49] Received request chatcmpl-e7502f16-5181-472c-ad85-7ce783889a10: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:46 [async_llm.py:382] Added request chatcmpl-e7502f16-5181-472c-ad85-7ce783889a10-936c9adb.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:46 [logger.py:49] Received request chatcmpl-6e458a52-cc5c-4a7c-8eb1-0656d26b6d29: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:46 [async_llm.py:382] Added request chatcmpl-6e458a52-cc5c-4a7c-8eb1-0656d26b6d29-ba541bae.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [encoder_cache_manager.py:118] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:46 [logger.py:49] Received request chatcmpl-68367714-9621-4be5-a7b4-a736e8be7e31: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:46 [async_llm.py:382] Added request chatcmpl-68367714-9621-4be5-a7b4-a736e8be7e31-9364d28c.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:46 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:47 [logger.py:49] Received request chatcmpl-a6ad0b1c-3bb9-4265-9325-72d39457695a: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:47 [async_llm.py:382] Added request chatcmpl-a6ad0b1c-3bb9-4265-9325-72d39457695a-a17a8770.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:47 [logger.py:49] Received request chatcmpl-2d931537-8dcc-4206-a8e2-cd1fe0f94527: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:47 [async_llm.py:382] Added request chatcmpl-2d931537-8dcc-4206-a8e2-cd1fe0f94527-89d3a252.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:47 [logger.py:49] Received request chatcmpl-1960f7df-4bde-4f0f-aa4d-d98444ffff8d: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:47 [async_llm.py:382] Added request chatcmpl-1960f7df-4bde-4f0f-aa4d-d98444ffff8d-b46a8228.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:47 [logger.py:49] Received request chatcmpl-524c2db4-0de5-476d-9e21-9ae1ff9bf72b: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:47 [async_llm.py:382] Added request chatcmpl-524c2db4-0de5-476d-9e21-9ae1ff9bf72b-88b27024.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:47 [logger.py:49] Received request chatcmpl-5a3af297-a09e-4102-a0fa-494a401d8dec: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:47 [async_llm.py:382] Added request chatcmpl-5a3af297-a09e-4102-a0fa-494a401d8dec-a3bed547.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [encoder_cache_manager.py:118] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:47 [logger.py:49] Received request chatcmpl-ba441256-38c0-4486-81fa-320a6b939ba9: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:47 [async_llm.py:382] Added request chatcmpl-ba441256-38c0-4486-81fa-320a6b939ba9-9c3914ea.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:47 [loggers.py:263] Engine 000: Avg prompt throughput: 5886.1 tokens/s, Avg generation throughput: 10.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 98.1%, Encoder cache hit rate: 98.1%
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:47 [logger.py:49] Received request chatcmpl-c8f41119-ab40-49f7-81bb-6c043437b985: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:47 [async_llm.py:382] Added request chatcmpl-c8f41119-ab40-49f7-81bb-6c043437b985-af21f5e1.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:47 [logger.py:49] Received request chatcmpl-4aa78b79-2e71-4d72-a30c-d6d9d17bd1b9: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:47 [async_llm.py:382] Added request chatcmpl-4aa78b79-2e71-4d72-a30c-d6d9d17bd1b9-9b35dd7c.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:47 [logger.py:49] Received request chatcmpl-4fbbbe45-4feb-45e8-b233-8a6e11bf8720: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:47 [async_llm.py:382] Added request chatcmpl-4fbbbe45-4feb-45e8-b233-8a6e11bf8720-84e9050e.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:47 [logger.py:49] Received request chatcmpl-363b7ac2-2584-45e4-a8b4-b6a88b9df127: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:47 [async_llm.py:382] Added request chatcmpl-363b7ac2-2584-45e4-a8b4-b6a88b9df127-8b0ca6be.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:47 [logger.py:49] Received request chatcmpl-8c1d9b26-90bc-4310-a4a3-34003844be45: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:47 [async_llm.py:382] Added request chatcmpl-8c1d9b26-90bc-4310-a4a3-34003844be45-83cfab81.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [encoder_cache_manager.py:118] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:47 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:48 [logger.py:49] Received request chatcmpl-bb88f6ac-6a6b-4d10-a919-cedf6871786b: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:48 [async_llm.py:382] Added request chatcmpl-bb88f6ac-6a6b-4d10-a919-cedf6871786b-ad1db0c0.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:48 [logger.py:49] Received request chatcmpl-72e709d6-7f06-4708-92ba-aa54751f6e82: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:48 [async_llm.py:382] Added request chatcmpl-72e709d6-7f06-4708-92ba-aa54751f6e82-bddebca7.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:48 [logger.py:49] Received request chatcmpl-893b9230-8d8e-476f-a480-7c0fe1adca95: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:48 [async_llm.py:382] Added request chatcmpl-893b9230-8d8e-476f-a480-7c0fe1adca95-88a774c1.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [encoder_cache_manager.py:118] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:48 [logger.py:49] Received request chatcmpl-a73bc56c-a17c-47bc-8368-c02f4e330a34: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:48 [async_llm.py:382] Added request chatcmpl-a73bc56c-a17c-47bc-8368-c02f4e330a34-948700fa.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:48 [logger.py:49] Received request chatcmpl-ea9a7e25-8aa4-40f6-bd44-b47de90b42c6: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:48 [async_llm.py:382] Added request chatcmpl-ea9a7e25-8aa4-40f6-bd44-b47de90b42c6-91f371f6.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:48 [logger.py:49] Received request chatcmpl-2a000796-78e7-4904-b101-986773078713: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:48 [async_llm.py:382] Added request chatcmpl-2a000796-78e7-4904-b101-986773078713-9dd09647.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [encoder_cache_manager.py:118] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:48 [logger.py:49] Received request chatcmpl-b05713c9-359f-4f54-9e9d-3ffec53b81ac: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:48 [async_llm.py:382] Added request chatcmpl-b05713c9-359f-4f54-9e9d-3ffec53b81ac-bce77195.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:48 [logger.py:49] Received request chatcmpl-0a69809d-6241-46df-8557-7fa2b93dccc3: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:48 [async_llm.py:382] Added request chatcmpl-0a69809d-6241-46df-8557-7fa2b93dccc3-a9b6818e.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:48 [logger.py:49] Received request chatcmpl-b7910b42-091a-4daf-9096-e39b5cb141d1: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:48 [async_llm.py:382] Added request chatcmpl-b7910b42-091a-4daf-9096-e39b5cb141d1-a635cce9.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:48 [logger.py:49] Received request chatcmpl-7b9cf00c-2860-4f77-b1b4-05eec2de4af5: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:48 [async_llm.py:382] Added request chatcmpl-7b9cf00c-2860-4f77-b1b4-05eec2de4af5-91cd06f6.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:48 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:49 [logger.py:49] Received request chatcmpl-e6c9727f-26c9-43f2-a2e1-35eeaf5ebd50: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:49 [async_llm.py:382] Added request chatcmpl-e6c9727f-26c9-43f2-a2e1-35eeaf5ebd50-8b06a2f6.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:49 [logger.py:49] Received request chatcmpl-7c79a08d-d2df-4610-aed0-3207fbf22513: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:49 [async_llm.py:382] Added request chatcmpl-7c79a08d-d2df-4610-aed0-3207fbf22513-bbb428ce.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:49 [logger.py:49] Received request chatcmpl-0fb185c7-b382-44a9-9fa7-42e5c33e7ac5: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:49 [async_llm.py:382] Added request chatcmpl-0fb185c7-b382-44a9-9fa7-42e5c33e7ac5-885a8190.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [encoder_cache_manager.py:118] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:49 [logger.py:49] Received request chatcmpl-2b3f7954-03ce-4671-9517-ecfe5e5c5084: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:49 [async_llm.py:382] Added request chatcmpl-2b3f7954-03ce-4671-9517-ecfe5e5c5084-9cd371de.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:49 [logger.py:49] Received request chatcmpl-3a396dff-8680-455d-92c5-116578b7b829: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:49 [async_llm.py:382] Added request chatcmpl-3a396dff-8680-455d-92c5-116578b7b829-8eec9914.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:49 [logger.py:49] Received request chatcmpl-d1e32e37-bed3-4212-9a50-ee7ecc054c70: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:49 [async_llm.py:382] Added request chatcmpl-d1e32e37-bed3-4212-9a50-ee7ecc054c70-9d6eb5dd.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:49 [logger.py:49] Received request chatcmpl-6330b9f3-551b-4a1b-aaf6-f9361ff8dd00: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:49 [async_llm.py:382] Added request chatcmpl-6330b9f3-551b-4a1b-aaf6-f9361ff8dd00-81c2a6a8.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:49 [logger.py:49] Received request chatcmpl-7e62f5ad-b96f-4cc1-8d5b-9b933ebf2f85: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:49 [async_llm.py:382] Added request chatcmpl-7e62f5ad-b96f-4cc1-8d5b-9b933ebf2f85-ad24f462.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:49 [logger.py:49] Received request chatcmpl-eab402ac-df5f-4423-a2a2-0f24b6ecfbf8: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:49 [async_llm.py:382] Added request chatcmpl-eab402ac-df5f-4423-a2a2-0f24b6ecfbf8-a0c0a6f4.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:49 [logger.py:49] Received request chatcmpl-c71a734f-e2e4-4be2-bdb9-d9d113b9d46c: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:49 [async_llm.py:382] Added request chatcmpl-c71a734f-e2e4-4be2-bdb9-d9d113b9d46c-b5e7b4cd.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [encoder_cache_manager.py:118] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:49 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:50 [logger.py:49] Received request chatcmpl-be5115c3-d7b6-420a-965c-7559f02875b6: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:50 [async_llm.py:382] Added request chatcmpl-be5115c3-d7b6-420a-965c-7559f02875b6-b7ef7cf5.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:50 [logger.py:49] Received request chatcmpl-80bebf42-9077-43eb-b30f-9cc5b998adda: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:50 [async_llm.py:382] Added request chatcmpl-80bebf42-9077-43eb-b30f-9cc5b998adda-a65d448f.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:50 [logger.py:49] Received request chatcmpl-32616bb8-ab66-47b9-9696-80ddaeabf596: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:50 [async_llm.py:382] Added request chatcmpl-32616bb8-ab66-47b9-9696-80ddaeabf596-a377cbf4.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:50 [logger.py:49] Received request chatcmpl-13232d6b-ee10-4206-bed9-b953f1b6a54a: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:50 [async_llm.py:382] Added request chatcmpl-13232d6b-ee10-4206-bed9-b953f1b6a54a-977b4d42.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:50 [logger.py:49] Received request chatcmpl-a8b5fa42-18a1-4621-b337-854a80e21687: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:50 [async_llm.py:382] Added request chatcmpl-a8b5fa42-18a1-4621-b337-854a80e21687-b73f0c12.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:50 [logger.py:49] Received request chatcmpl-5657ee12-98cb-4009-82ef-96e06f108a1e: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:50 [async_llm.py:382] Added request chatcmpl-5657ee12-98cb-4009-82ef-96e06f108a1e-99998e40.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [encoder_cache_manager.py:118] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:50 [logger.py:49] Received request chatcmpl-8e124bec-3895-4b17-9964-8ee2ddf7e008: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:50 [async_llm.py:382] Added request chatcmpl-8e124bec-3895-4b17-9964-8ee2ddf7e008-b3e6e9e2.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:50 [logger.py:49] Received request chatcmpl-f8d39daf-e84a-4de9-97b6-86aa515a356d: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:50 [async_llm.py:382] Added request chatcmpl-f8d39daf-e84a-4de9-97b6-86aa515a356d-8f097825.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:50 [logger.py:49] Received request chatcmpl-871b28b3-b003-4bf6-8a18-0bb1b6935cd0: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:50 [async_llm.py:382] Added request chatcmpl-871b28b3-b003-4bf6-8a18-0bb1b6935cd0-b8e30ee2.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:50 [logger.py:49] Received request chatcmpl-03925b97-7d0e-4aa0-963a-b7951622eaf1: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:50 [async_llm.py:382] Added request chatcmpl-03925b97-7d0e-4aa0-963a-b7951622eaf1-ab3c35f8.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [encoder_cache_manager.py:118] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:50 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:51 [logger.py:49] Received request chatcmpl-38440bc6-f04c-4aba-bd6e-a0a87db78e21: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:51 [async_llm.py:382] Added request chatcmpl-38440bc6-f04c-4aba-bd6e-a0a87db78e21-aa42ecb8.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [encoder_cache_manager.py:118] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:51 [logger.py:49] Received request chatcmpl-24b6bda8-bd0e-4ca8-b2cf-cc00e79bcd2c: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:51 [async_llm.py:382] Added request chatcmpl-24b6bda8-bd0e-4ca8-b2cf-cc00e79bcd2c-9459ce09.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:51 [logger.py:49] Received request chatcmpl-bd03f9e2-0acb-412a-8ba5-486c5afb9dd0: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:51 [async_llm.py:382] Added request chatcmpl-bd03f9e2-0acb-412a-8ba5-486c5afb9dd0-9fc2d99c.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:51 [logger.py:49] Received request chatcmpl-92d8da3e-6fb7-47a3-9381-4e10a5528f6b: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:51 [async_llm.py:382] Added request chatcmpl-92d8da3e-6fb7-47a3-9381-4e10a5528f6b-8ab0f579.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:51 [logger.py:49] Received request chatcmpl-51fb27ab-ce67-4c3f-aedc-bbfa4d912d5c: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:51 [async_llm.py:382] Added request chatcmpl-51fb27ab-ce67-4c3f-aedc-bbfa4d912d5c-8e085169.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:51 [logger.py:49] Received request chatcmpl-c17102bc-c848-46b0-bc8f-7ca87abff225: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:51 [async_llm.py:382] Added request chatcmpl-c17102bc-c848-46b0-bc8f-7ca87abff225-b3fae08d.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [encoder_cache_manager.py:118] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:51 [logger.py:49] Received request chatcmpl-07ecb503-d7ef-4dff-a9c1-e1bc8ed13b71: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:51 [async_llm.py:382] Added request chatcmpl-07ecb503-d7ef-4dff-a9c1-e1bc8ed13b71-a9ae977a.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:51 [logger.py:49] Received request chatcmpl-19abed76-fbb2-4f08-8770-9a2531a46547: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:51 [async_llm.py:382] Added request chatcmpl-19abed76-fbb2-4f08-8770-9a2531a46547-987cfdf2.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:51 [logger.py:49] Received request chatcmpl-732dc6ad-9c28-4f49-9ac5-05a1dddab297: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:51 [async_llm.py:382] Added request chatcmpl-732dc6ad-9c28-4f49-9ac5-05a1dddab297-acd50d48.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:51 [logger.py:49] Received request chatcmpl-66c9727c-1c72-45f3-a726-6f0887fe270a: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:51 [async_llm.py:382] Added request chatcmpl-66c9727c-1c72-45f3-a726-6f0887fe270a-a8906c66.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:51 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:52 [logger.py:49] Received request chatcmpl-cba5436c-4fae-49e8-b5cb-bac1aabe9e2e: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:52 [async_llm.py:382] Added request chatcmpl-cba5436c-4fae-49e8-b5cb-bac1aabe9e2e-9cff23fb.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:52 [logger.py:49] Received request chatcmpl-3c488d9f-908e-4809-a208-40a8db25d58b: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:52 [async_llm.py:382] Added request chatcmpl-3c488d9f-908e-4809-a208-40a8db25d58b-9c599316.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [encoder_cache_manager.py:118] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:52 [logger.py:49] Received request chatcmpl-fd41c943-8376-4b3b-bbc1-754f09b94e91: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:52 [async_llm.py:382] Added request chatcmpl-fd41c943-8376-4b3b-bbc1-754f09b94e91-b1ebe10d.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:52 [logger.py:49] Received request chatcmpl-25d66fa7-4c89-43f3-88a2-42e45497a3a8: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:52 [async_llm.py:382] Added request chatcmpl-25d66fa7-4c89-43f3-88a2-42e45497a3a8-9cbbb88c.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:52 [logger.py:49] Received request chatcmpl-852f2296-a845-4a03-b0de-9850b230f670: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:52 [async_llm.py:382] Added request chatcmpl-852f2296-a845-4a03-b0de-9850b230f670-ba5c4fe3.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:52 [logger.py:49] Received request chatcmpl-9706ec30-307c-4398-afb9-fa6a8b8bf84c: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:52 [async_llm.py:382] Added request chatcmpl-9706ec30-307c-4398-afb9-fa6a8b8bf84c-aa7c8383.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:52 [logger.py:49] Received request chatcmpl-eeb950ac-828c-49cc-8be9-b6c75b077369: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:52 [async_llm.py:382] Added request chatcmpl-eeb950ac-828c-49cc-8be9-b6c75b077369-93953e76.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:52 [logger.py:49] Received request chatcmpl-5fd79810-33f3-43e4-949c-18b9a7aa2aee: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:52 [async_llm.py:382] Added request chatcmpl-5fd79810-33f3-43e4-949c-18b9a7aa2aee-8d2db07e.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:52 [logger.py:49] Received request chatcmpl-7ddf593c-4c00-4568-a15a-962ac8c1ce3d: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:52 [async_llm.py:382] Added request chatcmpl-7ddf593c-4c00-4568-a15a-962ac8c1ce3d-81ae9c5e.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [encoder_cache_manager.py:118] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:52 [logger.py:49] Received request chatcmpl-f21f88e6-2a6b-4bb7-b4d5-cc46b44765ac: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:52 [async_llm.py:382] Added request chatcmpl-f21f88e6-2a6b-4bb7-b4d5-cc46b44765ac-a5690934.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:52 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:53 [logger.py:49] Received request chatcmpl-02300d04-ae03-49b1-976d-348a9c5ca986: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:53 [async_llm.py:382] Added request chatcmpl-02300d04-ae03-49b1-976d-348a9c5ca986-b2f8a7d9.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:53 [logger.py:49] Received request chatcmpl-983d380f-f26a-4f44-b34c-0b37ff455bec: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:53 [async_llm.py:382] Added request chatcmpl-983d380f-f26a-4f44-b34c-0b37ff455bec-99bea4a1.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:53 [logger.py:49] Received request chatcmpl-8829813d-aefd-445a-bbd5-e08c8a8d15ea: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:53 [async_llm.py:382] Added request chatcmpl-8829813d-aefd-445a-bbd5-e08c8a8d15ea-9156fcc8.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:53 [logger.py:49] Received request chatcmpl-6701b1a9-ff2c-4bff-8e8c-97b772c8050c: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:53 [async_llm.py:382] Added request chatcmpl-6701b1a9-ff2c-4bff-8e8c-97b772c8050c-a1cbef1f.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:53 [logger.py:49] Received request chatcmpl-e2d29467-909d-40a8-b193-c3901ec9403c: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:53 [async_llm.py:382] Added request chatcmpl-e2d29467-909d-40a8-b193-c3901ec9403c-86a2673a.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:53 [logger.py:49] Received request chatcmpl-dafa783d-20c6-479b-bc32-ddb949001a73: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:53 [async_llm.py:382] Added request chatcmpl-dafa783d-20c6-479b-bc32-ddb949001a73-9ebf1bff.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:53 [logger.py:49] Received request chatcmpl-17a6bdf0-deda-4f8f-a4cc-441d8e1cb36e: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:53 [async_llm.py:382] Added request chatcmpl-17a6bdf0-deda-4f8f-a4cc-441d8e1cb36e-86508d2e.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:53 [logger.py:49] Received request chatcmpl-5e3c2c56-5159-49d7-84e9-6466b7401af4: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:53 [async_llm.py:382] Added request chatcmpl-5e3c2c56-5159-49d7-84e9-6466b7401af4-a4b83dd7.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [encoder_cache_manager.py:118] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:53 [logger.py:49] Received request chatcmpl-7d307b36-328a-4a2c-992c-fed00404cfe2: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:53 [async_llm.py:382] Added request chatcmpl-7d307b36-328a-4a2c-992c-fed00404cfe2-9c50fc6e.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [encoder_cache_manager.py:118] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:53 [logger.py:49] Received request chatcmpl-b0de1027-7330-4ad6-8fcf-33cb170e9e91: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:53 [async_llm.py:382] Added request chatcmpl-b0de1027-7330-4ad6-8fcf-33cb170e9e91-bd61a6ae.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:53 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:53 [logger.py:49] Received request chatcmpl-f9dfae17-2169-45d4-9f95-8e6bc8dca110: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:54 [async_llm.py:382] Added request chatcmpl-f9dfae17-2169-45d4-9f95-8e6bc8dca110-a4faef80.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:54 [logger.py:49] Received request chatcmpl-56f977bf-a923-4545-b498-5bbcc8cfdec4: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:54 [async_llm.py:382] Added request chatcmpl-56f977bf-a923-4545-b498-5bbcc8cfdec4-84b437b0.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:54 [logger.py:49] Received request chatcmpl-97e8991f-d3a6-4bd5-8170-84d0c8b4d282: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:54 [async_llm.py:382] Added request chatcmpl-97e8991f-d3a6-4bd5-8170-84d0c8b4d282-aa8e75f8.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:54 [logger.py:49] Received request chatcmpl-bf01b483-2190-457e-88f3-b70420945087: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:54 [async_llm.py:382] Added request chatcmpl-bf01b483-2190-457e-88f3-b70420945087-afb456d0.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:54 [logger.py:49] Received request chatcmpl-2f4a4e42-70b4-4290-93b9-415fd42d6d16: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:54 [async_llm.py:382] Added request chatcmpl-2f4a4e42-70b4-4290-93b9-415fd42d6d16-971343cf.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:54 [logger.py:49] Received request chatcmpl-d862cfb9-4acd-4361-a509-3d4375aca82a: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:54 [async_llm.py:382] Added request chatcmpl-d862cfb9-4acd-4361-a509-3d4375aca82a-8531d25c.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:54 [logger.py:49] Received request chatcmpl-ec4d2456-277d-46a4-b1c4-d777696c8e55: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:54 [async_llm.py:382] Added request chatcmpl-ec4d2456-277d-46a4-b1c4-d777696c8e55-aafdd021.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:54 [logger.py:49] Received request chatcmpl-b6caaa3b-c7b2-4783-9e3c-4eff628b568f: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:54 [async_llm.py:382] Added request chatcmpl-b6caaa3b-c7b2-4783-9e3c-4eff628b568f-b74113d6.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:54 [logger.py:49] Received request chatcmpl-20fcd3b2-b326-46f7-a624-10abb1a865f6: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:54 [async_llm.py:382] Added request chatcmpl-20fcd3b2-b326-46f7-a624-10abb1a865f6-a8703df9.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [encoder_cache_manager.py:118] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:54 [logger.py:49] Received request chatcmpl-f623a1b4-2235-4a86-9514-f9de92e4e4be: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:54 [async_llm.py:382] Added request chatcmpl-f623a1b4-2235-4a86-9514-f9de92e4e4be-a52c9f29.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [encoder_cache_manager.py:118] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:54 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:54 [logger.py:49] Received request chatcmpl-f36ed7aa-f756-4da9-a37f-0a81b6105245: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:55 [async_llm.py:382] Added request chatcmpl-f36ed7aa-f756-4da9-a37f-0a81b6105245-a169f3df.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:55 [logger.py:49] Received request chatcmpl-e27445f8-cab9-4b01-a9a7-b047ed0fc996: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:55 [async_llm.py:382] Added request chatcmpl-e27445f8-cab9-4b01-a9a7-b047ed0fc996-94075edd.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:55 [logger.py:49] Received request chatcmpl-1b11c528-2e16-47fd-a327-6d7a94fed832: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:55 [async_llm.py:382] Added request chatcmpl-1b11c528-2e16-47fd-a327-6d7a94fed832-92c26c42.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [encoder_cache_manager.py:118] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:55 [logger.py:49] Received request chatcmpl-a7c0eb87-008c-494c-a589-af3ffbe0d32d: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:55 [async_llm.py:382] Added request chatcmpl-a7c0eb87-008c-494c-a589-af3ffbe0d32d-967f88dd.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [encoder_cache_manager.py:118] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:55 [logger.py:49] Received request chatcmpl-d75bfe8f-a60e-4025-bd54-8cbe8c6aeaa1: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:55 [async_llm.py:382] Added request chatcmpl-d75bfe8f-a60e-4025-bd54-8cbe8c6aeaa1-8d4d7f21.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:55 [logger.py:49] Received request chatcmpl-afb98691-7d61-4f06-8efc-378a694880d4: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:55 [async_llm.py:382] Added request chatcmpl-afb98691-7d61-4f06-8efc-378a694880d4-bf8c44b5.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:55 [logger.py:49] Received request chatcmpl-2a737410-0c96-4bbf-83bd-6963e68f4772: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:55 [async_llm.py:382] Added request chatcmpl-2a737410-0c96-4bbf-83bd-6963e68f4772-a74ffc9f.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:55 [logger.py:49] Received request chatcmpl-11543201-ffed-4820-80b1-362a252a131d: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:55 [async_llm.py:382] Added request chatcmpl-11543201-ffed-4820-80b1-362a252a131d-a57a6ef3.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:55 [logger.py:49] Received request chatcmpl-11623709-c981-4cf7-ad0d-00b6aa7ed3bf: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:55 [async_llm.py:382] Added request chatcmpl-11623709-c981-4cf7-ad0d-00b6aa7ed3bf-ae84ca0e.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:55 [logger.py:49] Received request chatcmpl-ec95adde-eb46-46e2-8360-c41428e398e3: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:55 [async_llm.py:382] Added request chatcmpl-ec95adde-eb46-46e2-8360-c41428e398e3-b460a84f.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:55 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:55 [logger.py:49] Received request chatcmpl-484cdfac-48ca-44d0-a6a5-843a884c171e: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:56 [async_llm.py:382] Added request chatcmpl-484cdfac-48ca-44d0-a6a5-843a884c171e-a7cc04a2.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:56 [logger.py:49] Received request chatcmpl-cbf713da-ff42-43e6-8e30-754f3c252c0a: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:56 [async_llm.py:382] Added request chatcmpl-cbf713da-ff42-43e6-8e30-754f3c252c0a-8cff8331.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:56 [logger.py:49] Received request chatcmpl-ecd4258a-800c-43a9-a683-73653653cda6: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:56 [async_llm.py:382] Added request chatcmpl-ecd4258a-800c-43a9-a683-73653653cda6-a1e91a7c.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:56 [logger.py:49] Received request chatcmpl-21e51406-68ea-4ed6-8476-eb04979df2d0: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:56 [async_llm.py:382] Added request chatcmpl-21e51406-68ea-4ed6-8476-eb04979df2d0-b74464b6.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:56 [logger.py:49] Received request chatcmpl-7613c4d8-03bf-47fc-a9e8-86d400006a27: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:56 [async_llm.py:382] Added request chatcmpl-7613c4d8-03bf-47fc-a9e8-86d400006a27-8f22c417.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:56 [logger.py:49] Received request chatcmpl-ae642c51-059e-4669-83de-44d60ef36b8f: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:56 [async_llm.py:382] Added request chatcmpl-ae642c51-059e-4669-83de-44d60ef36b8f-88ab1c7b.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:56 [logger.py:49] Received request chatcmpl-53e4b8da-5873-4d73-b1f0-f9a80e515f13: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:56 [async_llm.py:382] Added request chatcmpl-53e4b8da-5873-4d73-b1f0-f9a80e515f13-b7a8ffd1.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:56 [logger.py:49] Received request chatcmpl-2ef85441-8ae8-4d4e-871b-c643f0fe4dbe: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:56 [async_llm.py:382] Added request chatcmpl-2ef85441-8ae8-4d4e-871b-c643f0fe4dbe-86da8397.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:56 [logger.py:49] Received request chatcmpl-a76aaade-4ac1-42cc-8ef7-7d265fa80ca7: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:56 [async_llm.py:382] Added request chatcmpl-a76aaade-4ac1-42cc-8ef7-7d265fa80ca7-97939151.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:56 [logger.py:49] Received request chatcmpl-c84e1e54-4c62-4bda-851f-b730621506bc: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:56 [async_llm.py:382] Added request chatcmpl-c84e1e54-4c62-4bda-851f-b730621506bc-923f5e43.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:56 [logger.py:49] Received request chatcmpl-a6554eea-e79b-4666-9bb3-bd6b2cefb381: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:56 [async_llm.py:382] Added request chatcmpl-a6554eea-e79b-4666-9bb3-bd6b2cefb381-a0dc9cd7.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:56 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:57 [logger.py:49] Received request chatcmpl-20e90cad-417d-4a7e-aa34-d0a3b6d11373: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:57 [async_llm.py:382] Added request chatcmpl-20e90cad-417d-4a7e-aa34-d0a3b6d11373-943f004b.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [encoder_cache_manager.py:118] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:57 [logger.py:49] Received request chatcmpl-1d417484-b84f-455c-8093-bd356416637d: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:57 [async_llm.py:382] Added request chatcmpl-1d417484-b84f-455c-8093-bd356416637d-96919748.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:57 [logger.py:49] Received request chatcmpl-9e7705e9-ad5c-420c-baeb-a9f533717508: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:57 [async_llm.py:382] Added request chatcmpl-9e7705e9-ad5c-420c-baeb-a9f533717508-8a607222.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:57 [logger.py:49] Received request chatcmpl-a2431acc-a812-49ab-a931-7522fb5c8b7d: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:57 [async_llm.py:382] Added request chatcmpl-a2431acc-a812-49ab-a931-7522fb5c8b7d-b97052ec.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:57 [logger.py:49] Received request chatcmpl-04e81280-cad0-421b-b9da-92f6bfb09e89: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:57 [async_llm.py:382] Added request chatcmpl-04e81280-cad0-421b-b9da-92f6bfb09e89-9744fec7.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [encoder_cache_manager.py:118] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:57 [loggers.py:263] Engine 000: Avg prompt throughput: 5716.7 tokens/s, Avg generation throughput: 10.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 98.7%, Encoder cache hit rate: 98.7%
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:57 [logger.py:49] Received request chatcmpl-a43b00b0-be25-4037-98df-8f336f403990: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:57 [async_llm.py:382] Added request chatcmpl-a43b00b0-be25-4037-98df-8f336f403990-9b2b4a84.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [encoder_cache_manager.py:118] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:57 [logger.py:49] Received request chatcmpl-b2f885bb-8485-4adb-98ad-23a4be83018d: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:57 [async_llm.py:382] Added request chatcmpl-b2f885bb-8485-4adb-98ad-23a4be83018d-b3992da9.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:57 [logger.py:49] Received request chatcmpl-b1eafe1c-1518-4f31-b714-cbe8b6f623f3: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:57 [async_llm.py:382] Added request chatcmpl-b1eafe1c-1518-4f31-b714-cbe8b6f623f3-827e074e.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:57 [logger.py:49] Received request chatcmpl-6808ce92-bd91-4586-83c0-d70d6649dd11: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:57 [async_llm.py:382] Added request chatcmpl-6808ce92-bd91-4586-83c0-d70d6649dd11-9faf5145.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [encoder_cache_manager.py:118] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:57 [logger.py:49] Received request chatcmpl-65d31e5b-f06f-4c84-9998-d23673995961: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:57 [async_llm.py:382] Added request chatcmpl-65d31e5b-f06f-4c84-9998-d23673995961-8f732135.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:57 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:58 [logger.py:49] Received request chatcmpl-5da75bf9-1b1d-464c-aa1c-df8180937b25: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:58 [async_llm.py:382] Added request chatcmpl-5da75bf9-1b1d-464c-aa1c-df8180937b25-878bee7b.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:58 [logger.py:49] Received request chatcmpl-750ac22f-0998-4840-bdd4-4d89fa404268: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:58 [async_llm.py:382] Added request chatcmpl-750ac22f-0998-4840-bdd4-4d89fa404268-8c9e18cd.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:58 [logger.py:49] Received request chatcmpl-ede55f08-a8f8-4d35-a186-fd2cfd25655c: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:58 [async_llm.py:382] Added request chatcmpl-ede55f08-a8f8-4d35-a186-fd2cfd25655c-85836b96.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:58 [logger.py:49] Received request chatcmpl-ceca1382-467f-4e4f-ac06-2fcf3e6fb9a1: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:58 [async_llm.py:382] Added request chatcmpl-ceca1382-467f-4e4f-ac06-2fcf3e6fb9a1-90f93dc8.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [encoder_cache_manager.py:118] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:58 [logger.py:49] Received request chatcmpl-63d6cc8f-6aa8-402b-9345-42c97aefe2d8: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:58 [async_llm.py:382] Added request chatcmpl-63d6cc8f-6aa8-402b-9345-42c97aefe2d8-affb011b.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:58 [logger.py:49] Received request chatcmpl-7d1919e5-9708-46e1-b8d6-c19378b71fcb: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:58 [async_llm.py:382] Added request chatcmpl-7d1919e5-9708-46e1-b8d6-c19378b71fcb-8f905124.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:58 [logger.py:49] Received request chatcmpl-9afdfc41-912b-4e6b-9b2f-0ef8a273967c: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:58 [async_llm.py:382] Added request chatcmpl-9afdfc41-912b-4e6b-9b2f-0ef8a273967c-b5667d5b.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:58 [logger.py:49] Received request chatcmpl-b4838571-7bab-4932-a419-bc16d4a2f743: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:58 [async_llm.py:382] Added request chatcmpl-b4838571-7bab-4932-a419-bc16d4a2f743-9756906b.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:58 [logger.py:49] Received request chatcmpl-9f9ced71-1d6c-4f62-87b4-dca8b589f27b: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:58 [async_llm.py:382] Added request chatcmpl-9f9ced71-1d6c-4f62-87b4-dca8b589f27b-8594cfc3.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [encoder_cache_manager.py:118] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:58 [logger.py:49] Received request chatcmpl-15995050-0b04-4ca8-a463-857d7912ef91: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:58 [async_llm.py:382] Added request chatcmpl-15995050-0b04-4ca8-a463-857d7912ef91-84687e2e.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [encoder_cache_manager.py:118] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:58 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:59 [logger.py:49] Received request chatcmpl-44496844-d239-4fa4-ac4d-02114ea6a7d0: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:59 [async_llm.py:382] Added request chatcmpl-44496844-d239-4fa4-ac4d-02114ea6a7d0-ba6ffd04.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:59 [logger.py:49] Received request chatcmpl-ef7bd3c9-ac27-4132-96ce-e40f180aaf7c: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:59 [async_llm.py:382] Added request chatcmpl-ef7bd3c9-ac27-4132-96ce-e40f180aaf7c-8219455e.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:59 [logger.py:49] Received request chatcmpl-ce381c92-2867-41d3-a44e-c7ccacedca29: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:59 [async_llm.py:382] Added request chatcmpl-ce381c92-2867-41d3-a44e-c7ccacedca29-83b06c11.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:59 [logger.py:49] Received request chatcmpl-0b4a8123-0d85-4ee4-93ed-217e490e256a: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:59 [async_llm.py:382] Added request chatcmpl-0b4a8123-0d85-4ee4-93ed-217e490e256a-a858df0b.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:59 [logger.py:49] Received request chatcmpl-bb7c795c-6dbb-4b9e-b9f0-e08c4a21cc1e: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:59 [async_llm.py:382] Added request chatcmpl-bb7c795c-6dbb-4b9e-b9f0-e08c4a21cc1e-8827b053.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [encoder_cache_manager.py:118] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:59 [logger.py:49] Received request chatcmpl-a3ef3f27-9b6c-4643-8c3b-1570716ca956: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:59 [async_llm.py:382] Added request chatcmpl-a3ef3f27-9b6c-4643-8c3b-1570716ca956-8f2b17c2.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:59 [logger.py:49] Received request chatcmpl-211160aa-4ee5-4824-8a99-3061dc2b6b93: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:59 [async_llm.py:382] Added request chatcmpl-211160aa-4ee5-4824-8a99-3061dc2b6b93-b9e335cc.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:59 [logger.py:49] Received request chatcmpl-0c6cb440-c36a-4a20-8c66-ce5deed86337: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:59 [async_llm.py:382] Added request chatcmpl-0c6cb440-c36a-4a20-8c66-ce5deed86337-950f2dd6.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:59 [logger.py:49] Received request chatcmpl-9795566a-2c6c-4cf5-979f-bdb2f7554203: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:59 [async_llm.py:382] Added request chatcmpl-9795566a-2c6c-4cf5-979f-bdb2f7554203-8b5eb449.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [encoder_cache_manager.py:118] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:59 [logger.py:49] Received request chatcmpl-d053da29-5ebf-47f0-a68b-03cba07f84a6: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:32:59 [async_llm.py:382] Added request chatcmpl-d053da29-5ebf-47f0-a68b-03cba07f84a6-acfddd79.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:32:59 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:00 [logger.py:49] Received request chatcmpl-ac88c38d-ef05-4728-8c08-1f576a917250: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:00 [async_llm.py:382] Added request chatcmpl-ac88c38d-ef05-4728-8c08-1f576a917250-b1cedbf8.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:00 [logger.py:49] Received request chatcmpl-2d0f6071-1cd3-4b77-a5c7-094f6bd82225: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:00 [async_llm.py:382] Added request chatcmpl-2d0f6071-1cd3-4b77-a5c7-094f6bd82225-96b048e4.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [encoder_cache_manager.py:118] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:00 [logger.py:49] Received request chatcmpl-0214fc36-212c-485b-8cb9-41bb824d3f88: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:00 [async_llm.py:382] Added request chatcmpl-0214fc36-212c-485b-8cb9-41bb824d3f88-b14af322.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:00 [logger.py:49] Received request chatcmpl-69c80708-ab32-4290-9fbd-627bf2073393: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:00 [async_llm.py:382] Added request chatcmpl-69c80708-ab32-4290-9fbd-627bf2073393-a99ff2b4.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:00 [logger.py:49] Received request chatcmpl-aa43e3ff-b836-425e-878f-4606b742caa3: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:00 [async_llm.py:382] Added request chatcmpl-aa43e3ff-b836-425e-878f-4606b742caa3-907f9600.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:00 [logger.py:49] Received request chatcmpl-e7ea450f-e96f-48f9-94fc-f3d5a6a832a5: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:00 [async_llm.py:382] Added request chatcmpl-e7ea450f-e96f-48f9-94fc-f3d5a6a832a5-996eba3a.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:00 [logger.py:49] Received request chatcmpl-98519c30-5c5c-4e76-9394-0d214b8f5728: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:00 [async_llm.py:382] Added request chatcmpl-98519c30-5c5c-4e76-9394-0d214b8f5728-9077bcd7.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:00 [logger.py:49] Received request chatcmpl-a62cc55f-9936-435d-b1d7-7f9aae0279e0: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:00 [async_llm.py:382] Added request chatcmpl-a62cc55f-9936-435d-b1d7-7f9aae0279e0-b6c20106.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:00 [logger.py:49] Received request chatcmpl-36d9d0de-8dad-4599-857c-a4d4da5c1fe5: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:00 [async_llm.py:382] Added request chatcmpl-36d9d0de-8dad-4599-857c-a4d4da5c1fe5-98c1a33c.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [encoder_cache_manager.py:118] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:00 [logger.py:49] Received request chatcmpl-78383f39-0465-4dff-a7ba-6642105d0c1c: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:00 [async_llm.py:382] Added request chatcmpl-78383f39-0465-4dff-a7ba-6642105d0c1c-8a91b04b.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [encoder_cache_manager.py:118] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:00 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:01 [logger.py:49] Received request chatcmpl-73d5f773-0070-4a1d-a6a1-ca59f090cb9d: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:01 [async_llm.py:382] Added request chatcmpl-73d5f773-0070-4a1d-a6a1-ca59f090cb9d-8c9715e0.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:01 [logger.py:49] Received request chatcmpl-38402654-1cb3-461d-9436-03c74650e3bb: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:01 [async_llm.py:382] Added request chatcmpl-38402654-1cb3-461d-9436-03c74650e3bb-963a27a9.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:01 [logger.py:49] Received request chatcmpl-ae4f8c80-6e6b-4e22-b743-1a0ced460eaa: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:01 [async_llm.py:382] Added request chatcmpl-ae4f8c80-6e6b-4e22-b743-1a0ced460eaa-abb2246e.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:01 [logger.py:49] Received request chatcmpl-5235e529-7ae7-455c-8143-f22e45938d65: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:01 [async_llm.py:382] Added request chatcmpl-5235e529-7ae7-455c-8143-f22e45938d65-9ac6557a.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:01 [logger.py:49] Received request chatcmpl-04685383-bc08-45e1-8679-cf08265edd90: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:01 [async_llm.py:382] Added request chatcmpl-04685383-bc08-45e1-8679-cf08265edd90-89423b1d.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [encoder_cache_manager.py:118] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:01 [logger.py:49] Received request chatcmpl-03030757-6bd5-4ab0-acd2-166453fd32bc: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:01 [async_llm.py:382] Added request chatcmpl-03030757-6bd5-4ab0-acd2-166453fd32bc-88ad7f99.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:01 [logger.py:49] Received request chatcmpl-d5338284-c505-49db-acb7-9933fa42caba: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:01 [async_llm.py:382] Added request chatcmpl-d5338284-c505-49db-acb7-9933fa42caba-84415868.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:01 [logger.py:49] Received request chatcmpl-21362b13-7a8e-4856-92c0-4d3e994ffc13: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:01 [async_llm.py:382] Added request chatcmpl-21362b13-7a8e-4856-92c0-4d3e994ffc13-95db503b.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:01 [logger.py:49] Received request chatcmpl-2c02b649-901e-44f7-9c45-0aeb4672907f: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:01 [async_llm.py:382] Added request chatcmpl-2c02b649-901e-44f7-9c45-0aeb4672907f-8c336d66.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:01 [logger.py:49] Received request chatcmpl-8f734cfd-87cf-4376-8735-4fcf9633a5f6: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:01 [async_llm.py:382] Added request chatcmpl-8f734cfd-87cf-4376-8735-4fcf9633a5f6-b71fefef.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:01 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:02 [logger.py:49] Received request chatcmpl-f85a1432-5976-434a-9e57-6770bd21b83f: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:02 [async_llm.py:382] Added request chatcmpl-f85a1432-5976-434a-9e57-6770bd21b83f-9c01b01a.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:02 [logger.py:49] Received request chatcmpl-af1549b6-726d-4dcc-aac9-f97fa14c7d72: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:02 [async_llm.py:382] Added request chatcmpl-af1549b6-726d-4dcc-aac9-f97fa14c7d72-946f5206.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:02 [logger.py:49] Received request chatcmpl-67724a9f-84af-49b3-9169-73256782aa7f: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:02 [async_llm.py:382] Added request chatcmpl-67724a9f-84af-49b3-9169-73256782aa7f-b6373213.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:02 [logger.py:49] Received request chatcmpl-c743952e-4a36-4722-9590-71f320e91094: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:02 [async_llm.py:382] Added request chatcmpl-c743952e-4a36-4722-9590-71f320e91094-9b3e1be0.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:02 [logger.py:49] Received request chatcmpl-6f1534ef-9d8c-44ca-8c13-33e66e09efc8: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:02 [async_llm.py:382] Added request chatcmpl-6f1534ef-9d8c-44ca-8c13-33e66e09efc8-a4ed5012.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:02 [logger.py:49] Received request chatcmpl-65b2c642-b729-4257-bc3b-ac296c1c712a: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:02 [async_llm.py:382] Added request chatcmpl-65b2c642-b729-4257-bc3b-ac296c1c712a-845270eb.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:02 [logger.py:49] Received request chatcmpl-cb7c7da4-75ed-42f3-bcd2-f2192d4ac3ae: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:02 [async_llm.py:382] Added request chatcmpl-cb7c7da4-75ed-42f3-bcd2-f2192d4ac3ae-b044592b.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:02 [logger.py:49] Received request chatcmpl-20ed7384-55fc-4a84-a788-ec8ba2cfd87f: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:02 [async_llm.py:382] Added request chatcmpl-20ed7384-55fc-4a84-a788-ec8ba2cfd87f-ad936a23.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:02 [logger.py:49] Received request chatcmpl-147928d4-9eee-4787-98bd-6bd12ee861af: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:02 [async_llm.py:382] Added request chatcmpl-147928d4-9eee-4787-98bd-6bd12ee861af-a64781d5.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [encoder_cache_manager.py:118] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:02 [logger.py:49] Received request chatcmpl-f1618338-ef92-49fe-ad50-075fedd9de4e: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:02 [async_llm.py:382] Added request chatcmpl-f1618338-ef92-49fe-ad50-075fedd9de4e-8469aada.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:02 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:03 [logger.py:49] Received request chatcmpl-8ebf4441-2bf5-4596-846e-a87fd37e659f: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:03 [async_llm.py:382] Added request chatcmpl-8ebf4441-2bf5-4596-846e-a87fd37e659f-b94470d9.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:03 [logger.py:49] Received request chatcmpl-03efda50-622a-4700-9e9d-d42694802a2b: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:03 [async_llm.py:382] Added request chatcmpl-03efda50-622a-4700-9e9d-d42694802a2b-a96eab28.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:03 [logger.py:49] Received request chatcmpl-90537341-046e-49ab-ae11-52a47edf264b: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:03 [async_llm.py:382] Added request chatcmpl-90537341-046e-49ab-ae11-52a47edf264b-9f851d71.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:03 [logger.py:49] Received request chatcmpl-c0e30b91-8a79-4bac-a68b-1f9bb574709d: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:03 [async_llm.py:382] Added request chatcmpl-c0e30b91-8a79-4bac-a68b-1f9bb574709d-b2a06642.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [encoder_cache_manager.py:118] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:03 [logger.py:49] Received request chatcmpl-96920f6f-1be7-41c6-88b5-5e021afa2ce5: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:03 [async_llm.py:382] Added request chatcmpl-96920f6f-1be7-41c6-88b5-5e021afa2ce5-a33812ca.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:03 [logger.py:49] Received request chatcmpl-78d00e31-33f1-4fe4-acda-62172a791a1b: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:03 [async_llm.py:382] Added request chatcmpl-78d00e31-33f1-4fe4-acda-62172a791a1b-8734f1c3.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:03 [logger.py:49] Received request chatcmpl-3d099e5b-f3a1-498a-8acb-0ebff669b7c1: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:03 [async_llm.py:382] Added request chatcmpl-3d099e5b-f3a1-498a-8acb-0ebff669b7c1-a658850b.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [encoder_cache_manager.py:118] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:03 [logger.py:49] Received request chatcmpl-c95fac0c-830f-4bea-ac32-9eb4c11b3f77: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:03 [async_llm.py:382] Added request chatcmpl-c95fac0c-830f-4bea-ac32-9eb4c11b3f77-95094f08.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:03 [logger.py:49] Received request chatcmpl-8969917c-fad1-423b-a09d-86a545bf01d2: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:03 [async_llm.py:382] Added request chatcmpl-8969917c-fad1-423b-a09d-86a545bf01d2-b43bfae4.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [encoder_cache_manager.py:118] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:03 [logger.py:49] Received request chatcmpl-756854be-2186-4487-83e5-fa3982284774: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:03 [async_llm.py:382] Added request chatcmpl-756854be-2186-4487-83e5-fa3982284774-bcc8546a.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:03 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:04 [logger.py:49] Received request chatcmpl-c805d994-48bc-469a-988e-da9674267ae5: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:04 [async_llm.py:382] Added request chatcmpl-c805d994-48bc-469a-988e-da9674267ae5-8f1ef02b.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:04 [logger.py:49] Received request chatcmpl-d5d1f1cf-2d16-489e-b309-c134264424cb: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:04 [async_llm.py:382] Added request chatcmpl-d5d1f1cf-2d16-489e-b309-c134264424cb-bbbe85cc.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:04 [logger.py:49] Received request chatcmpl-a28004e8-ecee-43ea-aa69-f64c8baa1094: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:04 [async_llm.py:382] Added request chatcmpl-a28004e8-ecee-43ea-aa69-f64c8baa1094-b7122a24.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:04 [logger.py:49] Received request chatcmpl-d07e6dce-c66f-4b50-a290-50ca58b67da7: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:04 [async_llm.py:382] Added request chatcmpl-d07e6dce-c66f-4b50-a290-50ca58b67da7-b72dd859.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:04 [logger.py:49] Received request chatcmpl-0ef6b39e-5abb-4942-8761-5913ff5b5ee8: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:04 [async_llm.py:382] Added request chatcmpl-0ef6b39e-5abb-4942-8761-5913ff5b5ee8-9c7b65b2.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:04 [logger.py:49] Received request chatcmpl-b142adce-25de-4a1c-8a40-e9fbca24ee8c: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:04 [async_llm.py:382] Added request chatcmpl-b142adce-25de-4a1c-8a40-e9fbca24ee8c-a163665d.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:04 [logger.py:49] Received request chatcmpl-6cf848aa-81b4-46ec-82e3-62b6356b6071: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:04 [async_llm.py:382] Added request chatcmpl-6cf848aa-81b4-46ec-82e3-62b6356b6071-914768dd.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:04 [logger.py:49] Received request chatcmpl-ef81a0b7-c4f8-44e5-be60-5ed60519eb20: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:04 [async_llm.py:382] Added request chatcmpl-ef81a0b7-c4f8-44e5-be60-5ed60519eb20-baa018aa.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:04 [logger.py:49] Received request chatcmpl-59573ff9-70de-4689-bbaf-83fcabe33c9a: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:04 [async_llm.py:382] Added request chatcmpl-59573ff9-70de-4689-bbaf-83fcabe33c9a-9b972597.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:04 [logger.py:49] Received request chatcmpl-6beb8ca2-c825-4f21-84cb-856db347bbb7: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:04 [async_llm.py:382] Added request chatcmpl-6beb8ca2-c825-4f21-84cb-856db347bbb7-96ff2cbd.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:04 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:05 [logger.py:49] Received request chatcmpl-a2003908-56ab-4cbd-a110-16687beaae76: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:05 [async_llm.py:382] Added request chatcmpl-a2003908-56ab-4cbd-a110-16687beaae76-b8f61dca.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:05 [logger.py:49] Received request chatcmpl-dbe3dca3-757f-45d7-82ea-58442b6a83fa: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:05 [async_llm.py:382] Added request chatcmpl-dbe3dca3-757f-45d7-82ea-58442b6a83fa-ad38096f.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:05 [logger.py:49] Received request chatcmpl-58d3992b-b5b8-468c-ae43-8ed458626486: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:05 [async_llm.py:382] Added request chatcmpl-58d3992b-b5b8-468c-ae43-8ed458626486-b6dcce94.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:05 [logger.py:49] Received request chatcmpl-93d4d180-1e4d-4eec-90b4-528b111bab29: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:05 [async_llm.py:382] Added request chatcmpl-93d4d180-1e4d-4eec-90b4-528b111bab29-869c4144.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [encoder_cache_manager.py:118] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:05 [logger.py:49] Received request chatcmpl-8f416505-0dde-4f7f-be93-397c4de3fab0: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:05 [async_llm.py:382] Added request chatcmpl-8f416505-0dde-4f7f-be93-397c4de3fab0-b161cef4.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:05 [logger.py:49] Received request chatcmpl-d5d0e12e-20b5-49f7-8153-008445ed6dfe: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:05 [async_llm.py:382] Added request chatcmpl-d5d0e12e-20b5-49f7-8153-008445ed6dfe-ab53facf.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:05 [logger.py:49] Received request chatcmpl-374ffd13-a76e-4681-83bc-38179793673a: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:05 [async_llm.py:382] Added request chatcmpl-374ffd13-a76e-4681-83bc-38179793673a-95d84bd5.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [encoder_cache_manager.py:118] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:05 [logger.py:49] Received request chatcmpl-daa97887-89dd-46d9-bc89-fce81c06d808: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:05 [async_llm.py:382] Added request chatcmpl-daa97887-89dd-46d9-bc89-fce81c06d808-a572cfb4.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:05 [logger.py:49] Received request chatcmpl-b555feaa-608c-485b-81fe-db9e58d8e4f1: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:05 [async_llm.py:382] Added request chatcmpl-b555feaa-608c-485b-81fe-db9e58d8e4f1-927c604e.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:05 [logger.py:49] Received request chatcmpl-4d0ce5e3-2264-43a3-8aa9-298eb6621ec1: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:05 [async_llm.py:382] Added request chatcmpl-4d0ce5e3-2264-43a3-8aa9-298eb6621ec1-b0bd578e.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:05 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:06 [logger.py:49] Received request chatcmpl-3b0c0d98-fe6d-44bb-a332-aeea5d2274b9: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:06 [async_llm.py:382] Added request chatcmpl-3b0c0d98-fe6d-44bb-a332-aeea5d2274b9-989fa34b.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:06 [logger.py:49] Received request chatcmpl-fe146f14-14e6-4372-b3ff-e7a33438829a: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:06 [async_llm.py:382] Added request chatcmpl-fe146f14-14e6-4372-b3ff-e7a33438829a-92e80912.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:06 [logger.py:49] Received request chatcmpl-e516a775-262c-40ff-a327-73995fb13489: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:06 [async_llm.py:382] Added request chatcmpl-e516a775-262c-40ff-a327-73995fb13489-87ca4254.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:06 [logger.py:49] Received request chatcmpl-fd7a853d-a4e2-41e7-ad3b-7313fadc66df: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:06 [async_llm.py:382] Added request chatcmpl-fd7a853d-a4e2-41e7-ad3b-7313fadc66df-9cb71406.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:06 [logger.py:49] Received request chatcmpl-9b0fa876-e4f2-41fe-bf66-b525186f2f52: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:06 [async_llm.py:382] Added request chatcmpl-9b0fa876-e4f2-41fe-bf66-b525186f2f52-8d95712e.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:06 [logger.py:49] Received request chatcmpl-ee80759e-bd4d-4b97-8b2e-8747799fbfc0: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:06 [async_llm.py:382] Added request chatcmpl-ee80759e-bd4d-4b97-8b2e-8747799fbfc0-88794cab.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:06 [logger.py:49] Received request chatcmpl-2246a986-f7ae-447e-a9ca-5da36a5095be: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:06 [async_llm.py:382] Added request chatcmpl-2246a986-f7ae-447e-a9ca-5da36a5095be-8699ec9e.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:06 [logger.py:49] Received request chatcmpl-6a8294fa-cd65-479c-aceb-e2cb81e9476f: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:06 [async_llm.py:382] Added request chatcmpl-6a8294fa-cd65-479c-aceb-e2cb81e9476f-82615540.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:06 [logger.py:49] Received request chatcmpl-dc16a039-b1d7-4128-99c4-70c7ac08f7dc: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:06 [async_llm.py:382] Added request chatcmpl-dc16a039-b1d7-4128-99c4-70c7ac08f7dc-9809e8c2.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:06 [logger.py:49] Received request chatcmpl-a44655cc-cab7-4df8-9964-dcd3676d2ea5: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:06 [async_llm.py:382] Added request chatcmpl-a44655cc-cab7-4df8-9964-dcd3676d2ea5-a360dc81.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:06 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:07 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:07 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:07 [logger.py:49] Received request chatcmpl-8cea6874-2c6d-4d0b-8468-87406e7cf141: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:07 [async_llm.py:382] Added request chatcmpl-8cea6874-2c6d-4d0b-8468-87406e7cf141-9a4f5ec7.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:07 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:07 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:07 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:07 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:07 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:07 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:07 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:07 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:07 [logger.py:49] Received request chatcmpl-35cd95bc-7caa-45b4-8313-e257fe6c97ce: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:07 [async_llm.py:382] Added request chatcmpl-35cd95bc-7caa-45b4-8313-e257fe6c97ce-bacaa213.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:07 [encoder_cache_manager.py:118] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:07 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:07 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:07 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:07 [logger.py:49] Received request chatcmpl-a11a8333-5ecb-49ce-bb6f-e95318191b97: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:07 [async_llm.py:382] Added request chatcmpl-a11a8333-5ecb-49ce-bb6f-e95318191b97-9b856dc6.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:07 [encoder_cache_manager.py:118] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:07 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:07 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:07 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:07 [logger.py:49] Received request chatcmpl-1882eed2-5bb5-48b4-a2fa-46a7756012db: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:07 [async_llm.py:382] Added request chatcmpl-1882eed2-5bb5-48b4-a2fa-46a7756012db-852e2a00.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:07 [encoder_cache_manager.py:118] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:07 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:07 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:07 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:07 [logger.py:49] Received request chatcmpl-deb28b5e-fca7-49dd-a6cd-922df81143f2: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:07 [async_llm.py:382] Added request chatcmpl-deb28b5e-fca7-49dd-a6cd-922df81143f2-bf5d8e57.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:07 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:07 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:07 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:07 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:07 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:07 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:07 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:07 [loggers.py:263] Engine 000: Avg prompt throughput: 5660.0 tokens/s, Avg generation throughput: 10.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 99.0%, Encoder cache hit rate: 99.0%
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:07 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:07 [logger.py:49] Received request chatcmpl-019a79aa-6071-4130-8b05-10ba467f4de8: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:07 [async_llm.py:382] Added request chatcmpl-019a79aa-6071-4130-8b05-10ba467f4de8-8e6a9ff3.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:07 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:07 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:07 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:07 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:07 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:07 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:07 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:07 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:07 [logger.py:49] Received request chatcmpl-73b7ff71-085b-4e8f-aba1-77bee6569269: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:07 [async_llm.py:382] Added request chatcmpl-73b7ff71-085b-4e8f-aba1-77bee6569269-a8eebe7d.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:07 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:07 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:07 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:07 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:07 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:07 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:07 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:07 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:07 [logger.py:49] Received request chatcmpl-82a73060-c09e-4a2e-8db8-5b2e753de770: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:07 [async_llm.py:382] Added request chatcmpl-82a73060-c09e-4a2e-8db8-5b2e753de770-9b5dbe46.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:07 [encoder_cache_manager.py:118] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:07 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:07 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:07 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:07 [logger.py:49] Received request chatcmpl-9cbe1db7-a8d3-4f18-a35b-95f82e3c0816: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:07 [async_llm.py:382] Added request chatcmpl-9cbe1db7-a8d3-4f18-a35b-95f82e3c0816-8f4867da.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:07 [encoder_cache_manager.py:118] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:07 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:07 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:07 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:08 [logger.py:49] Received request chatcmpl-3c714f7e-21e0-4179-bf4c-50fc91ab59e9: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:08 [async_llm.py:382] Added request chatcmpl-3c714f7e-21e0-4179-bf4c-50fc91ab59e9-8cf8cbb8.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:08 [logger.py:49] Received request chatcmpl-88065866-2d56-4cfb-984a-ff45dcb8546a: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:08 [async_llm.py:382] Added request chatcmpl-88065866-2d56-4cfb-984a-ff45dcb8546a-8217e6e2.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:08 [logger.py:49] Received request chatcmpl-ec03f81f-de42-4fb5-b9f5-f98a0f6e723e: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:08 [async_llm.py:382] Added request chatcmpl-ec03f81f-de42-4fb5-b9f5-f98a0f6e723e-a7ed0ed4.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:08 [logger.py:49] Received request chatcmpl-58579efd-0132-4002-8b45-171160fb669a: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:08 [async_llm.py:382] Added request chatcmpl-58579efd-0132-4002-8b45-171160fb669a-bbb00d8a.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:08 [logger.py:49] Received request chatcmpl-1ee97d0f-40b1-4162-842a-bea0d9864e55: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:08 [async_llm.py:382] Added request chatcmpl-1ee97d0f-40b1-4162-842a-bea0d9864e55-8f0e386d.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:08 [logger.py:49] Received request chatcmpl-61f48372-4717-4927-bf2e-aaec4420e7ac: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:08 [async_llm.py:382] Added request chatcmpl-61f48372-4717-4927-bf2e-aaec4420e7ac-835d3054.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [encoder_cache_manager.py:118] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:08 [logger.py:49] Received request chatcmpl-c7cfa616-5d65-4cec-8a4b-eb5997cc2c51: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:08 [async_llm.py:382] Added request chatcmpl-c7cfa616-5d65-4cec-8a4b-eb5997cc2c51-90819cca.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:08 [logger.py:49] Received request chatcmpl-fc0fd657-a1a5-444d-aa72-c4e300903511: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:08 [async_llm.py:382] Added request chatcmpl-fc0fd657-a1a5-444d-aa72-c4e300903511-9cf18d14.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [encoder_cache_manager.py:118] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:08 [logger.py:49] Received request chatcmpl-6a34291e-775f-4b9a-a197-8e7193cbfe59: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:08 [async_llm.py:382] Added request chatcmpl-6a34291e-775f-4b9a-a197-8e7193cbfe59-b690893d.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:08 [logger.py:49] Received request chatcmpl-63e59a3c-2b01-4a35-ba1a-ab86f5025829: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:08 [async_llm.py:382] Added request chatcmpl-63e59a3c-2b01-4a35-ba1a-ab86f5025829-b2313be1.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:08 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:09 [logger.py:49] Received request chatcmpl-a919bed1-50ad-4940-9b3b-b56525bbe107: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:09 [async_llm.py:382] Added request chatcmpl-a919bed1-50ad-4940-9b3b-b56525bbe107-9aaf24db.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [encoder_cache_manager.py:118] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:09 [logger.py:49] Received request chatcmpl-297e4f6a-6934-4cd4-b422-ae34ce14bd07: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:09 [async_llm.py:382] Added request chatcmpl-297e4f6a-6934-4cd4-b422-ae34ce14bd07-b06f77db.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [encoder_cache_manager.py:118] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:09 [logger.py:49] Received request chatcmpl-edfd028d-2c15-47a8-873d-1f3616bfefe6: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:09 [async_llm.py:382] Added request chatcmpl-edfd028d-2c15-47a8-873d-1f3616bfefe6-ac452325.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:09 [logger.py:49] Received request chatcmpl-87e9d766-b40c-4186-94a1-458823c670ab: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:09 [async_llm.py:382] Added request chatcmpl-87e9d766-b40c-4186-94a1-458823c670ab-9f0aa0ac.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:09 [logger.py:49] Received request chatcmpl-914448b2-f26b-4a59-b35a-100938d385b9: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:09 [async_llm.py:382] Added request chatcmpl-914448b2-f26b-4a59-b35a-100938d385b9-a83cd0f0.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [encoder_cache_manager.py:118] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:09 [logger.py:49] Received request chatcmpl-0875fe35-c236-4574-b3af-5d515f6800f3: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:09 [async_llm.py:382] Added request chatcmpl-0875fe35-c236-4574-b3af-5d515f6800f3-932694a8.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:09 [logger.py:49] Received request chatcmpl-cad5f810-f06a-457b-a67b-ed3cbfbeec9b: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:09 [async_llm.py:382] Added request chatcmpl-cad5f810-f06a-457b-a67b-ed3cbfbeec9b-a14b2687.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [encoder_cache_manager.py:118] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:09 [logger.py:49] Received request chatcmpl-edf6def9-4772-438f-87c8-74c40ea15871: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:09 [async_llm.py:382] Added request chatcmpl-edf6def9-4772-438f-87c8-74c40ea15871-b69efe3a.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:09 [logger.py:49] Received request chatcmpl-551f0dc1-f5e3-4bd6-8ae3-0a4a54bcb51c: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:09 [async_llm.py:382] Added request chatcmpl-551f0dc1-f5e3-4bd6-8ae3-0a4a54bcb51c-88bfec89.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:09 [logger.py:49] Received request chatcmpl-71b418f9-858e-4c6a-8737-4a455165e562: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:09 [async_llm.py:382] Added request chatcmpl-71b418f9-858e-4c6a-8737-4a455165e562-80533d3b.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:09 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:10 [logger.py:49] Received request chatcmpl-6cef2b35-03ed-496a-a7ed-c2f4cb132bbb: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:10 [async_llm.py:382] Added request chatcmpl-6cef2b35-03ed-496a-a7ed-c2f4cb132bbb-b7cd615e.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:10 [logger.py:49] Received request chatcmpl-cd215e83-ea4f-4983-9d35-4515bfe58813: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:10 [async_llm.py:382] Added request chatcmpl-cd215e83-ea4f-4983-9d35-4515bfe58813-916d218b.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:10 [logger.py:49] Received request chatcmpl-348d2404-825b-4649-81d8-0fe0fa98ffe3: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:10 [async_llm.py:382] Added request chatcmpl-348d2404-825b-4649-81d8-0fe0fa98ffe3-94f82bdd.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [encoder_cache_manager.py:118] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:10 [logger.py:49] Received request chatcmpl-8279cc09-369c-4bcd-980c-2ff06e91680c: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:10 [async_llm.py:382] Added request chatcmpl-8279cc09-369c-4bcd-980c-2ff06e91680c-87dab85c.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:10 [logger.py:49] Received request chatcmpl-3f17622a-8657-4c8f-9818-9eccd94c0b7e: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:10 [async_llm.py:382] Added request chatcmpl-3f17622a-8657-4c8f-9818-9eccd94c0b7e-87af2549.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:10 [logger.py:49] Received request chatcmpl-21cd7e22-48a8-46fe-9e00-cc5c5e461f22: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:10 [async_llm.py:382] Added request chatcmpl-21cd7e22-48a8-46fe-9e00-cc5c5e461f22-89cac9ef.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:10 [logger.py:49] Received request chatcmpl-fb6f6f9a-9250-47dc-b725-1c53b6a0d03e: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:10 [async_llm.py:382] Added request chatcmpl-fb6f6f9a-9250-47dc-b725-1c53b6a0d03e-bebef3f5.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:10 [logger.py:49] Received request chatcmpl-444deed2-0ef7-40ce-80b5-29aef80687cc: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:10 [async_llm.py:382] Added request chatcmpl-444deed2-0ef7-40ce-80b5-29aef80687cc-832044d7.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:10 [logger.py:49] Received request chatcmpl-0d7f1bd2-1a9f-4f00-879f-99a86f688dc9: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:10 [async_llm.py:382] Added request chatcmpl-0d7f1bd2-1a9f-4f00-879f-99a86f688dc9-9088c19e.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:10 [logger.py:49] Received request chatcmpl-1364a136-cefd-455f-94d4-4e892aa53594: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:10 [async_llm.py:382] Added request chatcmpl-1364a136-cefd-455f-94d4-4e892aa53594-82c4a4b4.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [encoder_cache_manager.py:118] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:10 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:11 [logger.py:49] Received request chatcmpl-bcf8e135-c795-4a0f-a944-a9675de268b9: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:11 [async_llm.py:382] Added request chatcmpl-bcf8e135-c795-4a0f-a944-a9675de268b9-a3be3231.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:11 [logger.py:49] Received request chatcmpl-6f9d5f9a-c4ef-459c-bd8a-b88a6966017c: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:11 [async_llm.py:382] Added request chatcmpl-6f9d5f9a-c4ef-459c-bd8a-b88a6966017c-a1528b3e.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [encoder_cache_manager.py:118] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:11 [logger.py:49] Received request chatcmpl-623e0656-e4f6-460e-a9d6-32ea4e8b5416: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:11 [async_llm.py:382] Added request chatcmpl-623e0656-e4f6-460e-a9d6-32ea4e8b5416-bd2d2584.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [encoder_cache_manager.py:118] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:11 [logger.py:49] Received request chatcmpl-a053ca84-e229-4b1c-ac48-e50313221660: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:11 [async_llm.py:382] Added request chatcmpl-a053ca84-e229-4b1c-ac48-e50313221660-8cf3d74d.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [encoder_cache_manager.py:118] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:11 [logger.py:49] Received request chatcmpl-0a6ef31e-98a0-4409-a5ac-8a905e2533b1: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:11 [async_llm.py:382] Added request chatcmpl-0a6ef31e-98a0-4409-a5ac-8a905e2533b1-9f4ba87c.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:11 [logger.py:49] Received request chatcmpl-64b5522e-f533-4b2f-a039-4aca013ff723: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:11 [async_llm.py:382] Added request chatcmpl-64b5522e-f533-4b2f-a039-4aca013ff723-a38388a7.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:11 [logger.py:49] Received request chatcmpl-c577f9b8-8c55-41ba-865b-d5b4fa6ea223: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:11 [async_llm.py:382] Added request chatcmpl-c577f9b8-8c55-41ba-865b-d5b4fa6ea223-8e9373ab.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:11 [logger.py:49] Received request chatcmpl-83bed6bb-dbe3-4d91-9878-345ff697d708: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:11 [async_llm.py:382] Added request chatcmpl-83bed6bb-dbe3-4d91-9878-345ff697d708-af515ab4.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:11 [logger.py:49] Received request chatcmpl-d47da3ed-e93d-40b6-a264-034333996e5c: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:11 [async_llm.py:382] Added request chatcmpl-d47da3ed-e93d-40b6-a264-034333996e5c-b5c7fb6e.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:11 [logger.py:49] Received request chatcmpl-2cb436e2-740a-4999-a025-3a41d2a83d83: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:11 [async_llm.py:382] Added request chatcmpl-2cb436e2-740a-4999-a025-3a41d2a83d83-992ca9e6.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:11 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:12 [logger.py:49] Received request chatcmpl-46d66590-2747-40b4-b03d-b4b15ad315bb: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:12 [async_llm.py:382] Added request chatcmpl-46d66590-2747-40b4-b03d-b4b15ad315bb-84909c47.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:12 [logger.py:49] Received request chatcmpl-e8e5a003-1fd6-4350-a5c2-cc60595afa7e: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:12 [async_llm.py:382] Added request chatcmpl-e8e5a003-1fd6-4350-a5c2-cc60595afa7e-850a0b1f.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:12 [logger.py:49] Received request chatcmpl-a067ce33-430c-412b-9786-2780d2415982: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:12 [async_llm.py:382] Added request chatcmpl-a067ce33-430c-412b-9786-2780d2415982-bdea2eb3.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:12 [logger.py:49] Received request chatcmpl-91e40a93-d250-4073-86e2-5404f3077ff5: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:12 [async_llm.py:382] Added request chatcmpl-91e40a93-d250-4073-86e2-5404f3077ff5-b4d63ea7.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:12 [logger.py:49] Received request chatcmpl-d17d892f-feec-4f81-91ac-b951164ef5db: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:12 [async_llm.py:382] Added request chatcmpl-d17d892f-feec-4f81-91ac-b951164ef5db-a10b515c.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:12 [logger.py:49] Received request chatcmpl-467b595f-3f03-4101-a83a-37948ddf70d7: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:12 [async_llm.py:382] Added request chatcmpl-467b595f-3f03-4101-a83a-37948ddf70d7-b2fc98f2.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [encoder_cache_manager.py:118] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:12 [logger.py:49] Received request chatcmpl-137a5b74-d3f3-4e40-9b54-3f5621016042: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:12 [async_llm.py:382] Added request chatcmpl-137a5b74-d3f3-4e40-9b54-3f5621016042-935851ff.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:12 [logger.py:49] Received request chatcmpl-85ed448d-1728-46e0-8d6f-f330b41d66f4: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:12 [async_llm.py:382] Added request chatcmpl-85ed448d-1728-46e0-8d6f-f330b41d66f4-8daff682.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:12 [logger.py:49] Received request chatcmpl-a573df44-5623-4996-83d6-07dad0bb1e89: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:12 [async_llm.py:382] Added request chatcmpl-a573df44-5623-4996-83d6-07dad0bb1e89-acde7eb5.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:12 [logger.py:49] Received request chatcmpl-416bd6df-2059-46b9-9f43-835571c00eea: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:12 [async_llm.py:382] Added request chatcmpl-416bd6df-2059-46b9-9f43-835571c00eea-bddb0836.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:12 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:13 [logger.py:49] Received request chatcmpl-c26257c8-43c5-4d9e-920f-1f0601c363a1: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:13 [async_llm.py:382] Added request chatcmpl-c26257c8-43c5-4d9e-920f-1f0601c363a1-a2a73d2a.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [encoder_cache_manager.py:118] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:13 [logger.py:49] Received request chatcmpl-d07b7367-7cd4-4dd1-a7a6-838f7632ea97: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:13 [async_llm.py:382] Added request chatcmpl-d07b7367-7cd4-4dd1-a7a6-838f7632ea97-a814e99f.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [encoder_cache_manager.py:118] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:13 [logger.py:49] Received request chatcmpl-3128cb2f-99fe-4e34-9e28-bf6bff9b6b23: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:13 [async_llm.py:382] Added request chatcmpl-3128cb2f-99fe-4e34-9e28-bf6bff9b6b23-8b99b613.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:13 [logger.py:49] Received request chatcmpl-bd6b647d-6637-4f88-bacd-ef88b986d2ae: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:13 [async_llm.py:382] Added request chatcmpl-bd6b647d-6637-4f88-bacd-ef88b986d2ae-96f1047d.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:13 [logger.py:49] Received request chatcmpl-9ce6bf01-c43d-4838-94e4-b9cdb4db980d: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:13 [async_llm.py:382] Added request chatcmpl-9ce6bf01-c43d-4838-94e4-b9cdb4db980d-b80f70d2.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:13 [logger.py:49] Received request chatcmpl-59830d19-057f-412d-bdbe-1f1690228661: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:13 [async_llm.py:382] Added request chatcmpl-59830d19-057f-412d-bdbe-1f1690228661-b2baa5c1.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:13 [logger.py:49] Received request chatcmpl-0d8079c7-3609-4a67-a135-8d62383da7c3: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:13 [async_llm.py:382] Added request chatcmpl-0d8079c7-3609-4a67-a135-8d62383da7c3-90289c6c.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:13 [logger.py:49] Received request chatcmpl-e4a5801e-260f-4d0d-993d-4a9a18297faa: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:13 [async_llm.py:382] Added request chatcmpl-e4a5801e-260f-4d0d-993d-4a9a18297faa-810c3c1f.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [encoder_cache_manager.py:118] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:13 [logger.py:49] Received request chatcmpl-658b23e1-584e-4300-9ea8-f2fe0fb30974: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:13 [async_llm.py:382] Added request chatcmpl-658b23e1-584e-4300-9ea8-f2fe0fb30974-9b744faa.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:13 [logger.py:49] Received request chatcmpl-35c52ab4-166e-429e-ae6d-ada4a83ad0e7: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:13 [async_llm.py:382] Added request chatcmpl-35c52ab4-166e-429e-ae6d-ada4a83ad0e7-86a3e6cd.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:13 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:14 [logger.py:49] Received request chatcmpl-47df2403-0ab6-4c82-acc4-4ea9ea514d34: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:14 [async_llm.py:382] Added request chatcmpl-47df2403-0ab6-4c82-acc4-4ea9ea514d34-908ce0ea.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:14 [logger.py:49] Received request chatcmpl-6ceb3790-6902-4dba-a83b-a483693b9d35: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:14 [async_llm.py:382] Added request chatcmpl-6ceb3790-6902-4dba-a83b-a483693b9d35-817dba40.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:14 [logger.py:49] Received request chatcmpl-6b55032b-ceb9-41e7-96dc-8033ff2f92d4: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:14 [async_llm.py:382] Added request chatcmpl-6b55032b-ceb9-41e7-96dc-8033ff2f92d4-b19e73a4.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:14 [logger.py:49] Received request chatcmpl-4f07efb6-f5b8-4a0b-8224-03b6be3fe0a2: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:14 [async_llm.py:382] Added request chatcmpl-4f07efb6-f5b8-4a0b-8224-03b6be3fe0a2-9e2bff95.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:14 [logger.py:49] Received request chatcmpl-7b07ea54-24e3-47c8-add1-f24fed917c1e: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:14 [async_llm.py:382] Added request chatcmpl-7b07ea54-24e3-47c8-add1-f24fed917c1e-ab5898ae.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:14 [logger.py:49] Received request chatcmpl-70cd5202-30e6-4043-ab12-ae6013c4ac20: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:14 [async_llm.py:382] Added request chatcmpl-70cd5202-30e6-4043-ab12-ae6013c4ac20-8923a602.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:14 [logger.py:49] Received request chatcmpl-dc328c9f-a9f4-4a58-a9a6-a28f81ee8571: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:14 [async_llm.py:382] Added request chatcmpl-dc328c9f-a9f4-4a58-a9a6-a28f81ee8571-a14f3269.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:14 [logger.py:49] Received request chatcmpl-88caf96b-bd81-474c-9b65-1b3e66bf4464: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:14 [async_llm.py:382] Added request chatcmpl-88caf96b-bd81-474c-9b65-1b3e66bf4464-a319c277.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:14 [logger.py:49] Received request chatcmpl-c6da9800-3aff-4c44-815c-32adad7a921a: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:14 [async_llm.py:382] Added request chatcmpl-c6da9800-3aff-4c44-815c-32adad7a921a-8e7faed5.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:14 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:15 [logger.py:49] Received request chatcmpl-a28ff6e9-8fcd-4e55-8aaf-43f56d0f9e6e: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:15 [async_llm.py:382] Added request chatcmpl-a28ff6e9-8fcd-4e55-8aaf-43f56d0f9e6e-99a2c8f1.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:15 [logger.py:49] Received request chatcmpl-a5041568-684c-4ce2-a2c3-99a62f680f93: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:15 [async_llm.py:382] Added request chatcmpl-a5041568-684c-4ce2-a2c3-99a62f680f93-950cf70e.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [encoder_cache_manager.py:118] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:15 [logger.py:49] Received request chatcmpl-b1147dfe-72d6-47f5-870d-c1645a9d2160: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:15 [async_llm.py:382] Added request chatcmpl-b1147dfe-72d6-47f5-870d-c1645a9d2160-80c0af3e.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:15 [logger.py:49] Received request chatcmpl-76b4c3fc-e4d4-4387-809b-7bfc3325682e: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:15 [async_llm.py:382] Added request chatcmpl-76b4c3fc-e4d4-4387-809b-7bfc3325682e-8513520d.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [encoder_cache_manager.py:118] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:15 [logger.py:49] Received request chatcmpl-ccf8128d-4794-4b7c-81b9-a35f16be8f30: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:15 [async_llm.py:382] Added request chatcmpl-ccf8128d-4794-4b7c-81b9-a35f16be8f30-8d9d8d28.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [encoder_cache_manager.py:118] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:15 [logger.py:49] Received request chatcmpl-4c664c60-4571-4ae3-83b1-4f4edd1025a3: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:15 [async_llm.py:382] Added request chatcmpl-4c664c60-4571-4ae3-83b1-4f4edd1025a3-b75a1cb0.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:15 [logger.py:49] Received request chatcmpl-34065d6e-9c1e-4597-bcf1-1040b42d9ecc: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:15 [async_llm.py:382] Added request chatcmpl-34065d6e-9c1e-4597-bcf1-1040b42d9ecc-8b8d398e.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [encoder_cache_manager.py:118] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:15 [logger.py:49] Received request chatcmpl-c1b3182c-24d6-43cf-b8f7-2a57284be488: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:15 [async_llm.py:382] Added request chatcmpl-c1b3182c-24d6-43cf-b8f7-2a57284be488-86bfb2e0.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:15 [logger.py:49] Received request chatcmpl-51fd6dbc-fff6-40db-b51e-6914a6be7b58: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:15 [async_llm.py:382] Added request chatcmpl-51fd6dbc-fff6-40db-b51e-6914a6be7b58-823b3ca6.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:15 [logger.py:49] Received request chatcmpl-9c0346d3-ab32-4061-9135-a4febecc2516: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:15 [async_llm.py:382] Added request chatcmpl-9c0346d3-ab32-4061-9135-a4febecc2516-a36b8a9c.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:15 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:16 [logger.py:49] Received request chatcmpl-88c79701-310b-4511-aebc-fa8b4dd256a3: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:16 [async_llm.py:382] Added request chatcmpl-88c79701-310b-4511-aebc-fa8b4dd256a3-98d98612.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:16 [logger.py:49] Received request chatcmpl-53c35ddd-9696-4233-a51d-c667a9d89f4a: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:16 [async_llm.py:382] Added request chatcmpl-53c35ddd-9696-4233-a51d-c667a9d89f4a-9e8f8dae.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [encoder_cache_manager.py:118] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:16 [logger.py:49] Received request chatcmpl-736320dd-a3df-4fdd-bc64-9dc2ff25fc21: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:16 [async_llm.py:382] Added request chatcmpl-736320dd-a3df-4fdd-bc64-9dc2ff25fc21-b3d29791.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [encoder_cache_manager.py:118] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:16 [logger.py:49] Received request chatcmpl-bf73ca68-a583-41ae-a195-64ae3db91e49: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:16 [async_llm.py:382] Added request chatcmpl-bf73ca68-a583-41ae-a195-64ae3db91e49-861f4ec4.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:16 [logger.py:49] Received request chatcmpl-cf60c4d8-1f11-43d8-b376-cd4c96cda16b: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:16 [async_llm.py:382] Added request chatcmpl-cf60c4d8-1f11-43d8-b376-cd4c96cda16b-ab5e2d3b.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:16 [logger.py:49] Received request chatcmpl-9c4314ba-4bd7-4d4b-8605-d516426c5ad0: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:16 [async_llm.py:382] Added request chatcmpl-9c4314ba-4bd7-4d4b-8605-d516426c5ad0-98f4d561.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:16 [logger.py:49] Received request chatcmpl-fe2c9682-3bc6-400d-b5bd-2d959719d6e1: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:16 [async_llm.py:382] Added request chatcmpl-fe2c9682-3bc6-400d-b5bd-2d959719d6e1-b7bc8f1d.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:16 [logger.py:49] Received request chatcmpl-4d3083e3-a9ae-4e03-95e8-9b0d406356b9: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:16 [async_llm.py:382] Added request chatcmpl-4d3083e3-a9ae-4e03-95e8-9b0d406356b9-b6cea9d0.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [encoder_cache_manager.py:118] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached in GPU cache but not referenced by any request
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:16 [logger.py:49] Received request chatcmpl-a2e1b843-1539-424a-904b-fc1ea356430d: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:16 [async_llm.py:382] Added request chatcmpl-a2e1b843-1539-424a-904b-fc1ea356430d-bb025e29.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:16 [logger.py:49] Received request chatcmpl-0dd20891-a15f-4d7f-a1ed-aaa41793fac2: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:16 [async_llm.py:382] Added request chatcmpl-0dd20891-a15f-4d7f-a1ed-aaa41793fac2-abf8afff.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:16 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:17 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:17 [logger.py:49] Received request chatcmpl-a9db763a-05e3-4622-a1c8-e33231693b5d: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:17 [async_llm.py:382] Added request chatcmpl-a9db763a-05e3-4622-a1c8-e33231693b5d-9626f143.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:17 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:17 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:17 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:17 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:17 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:17 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:17 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:17 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:17 [logger.py:49] Received request chatcmpl-88115af4-8cc3-4ad0-98c2-616be61035f7: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:17 [async_llm.py:382] Added request chatcmpl-88115af4-8cc3-4ad0-98c2-616be61035f7-82bd1a85.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:17 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:17 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:17 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:17 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:17 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:17 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:17 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:17 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:17 [logger.py:49] Received request chatcmpl-4b3b06e7-1874-44c8-8c99-a67a4c2af9e7: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:17 [async_llm.py:382] Added request chatcmpl-4b3b06e7-1874-44c8-8c99-a67a4c2af9e7-b6d004ca.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:17 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:17 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:17 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:17 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:17 [gpu_model_runner.py:871] Freeing encoder cache for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:17 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:17 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:17 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:17 [logger.py:49] Received request chatcmpl-2ef7b4e7-cd87-4445-bdd8-825706bde677: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:17 [async_llm.py:382] Added request chatcmpl-2ef7b4e7-cd87-4445-bdd8-825706bde677-9e32ece5.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:17 [encoder_cache_manager.py:126] mm_hash 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:17 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:17 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:17 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:17 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:17 [gpu_model_runner.py:2404] Encoder cache miss for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:17 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:17 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:17 [logger.py:49] Received request chatcmpl-21af4f48-ffed-4df3-baa2-b308b4e1fede: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:17 [async_llm.py:382] Added request chatcmpl-21af4f48-ffed-4df3-baa2-b308b4e1fede-8806492c.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:17 [encoder_cache_manager.py:126] mm_hash 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828 is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:17 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:17 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:17 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:17 [gpu_model_runner.py:871] Freeing encoder cache for 9ceffa12d40e557a4a6edf312f0e3c2af1d7bc228683a2f149db8f35d27e2ff5
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:17 [gpu_model_runner.py:2404] Encoder cache miss for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:17 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:17 [loggers.py:263] Engine 000: Avg prompt throughput: 5546.4 tokens/s, Avg generation throughput: 9.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 99.2%, Encoder cache hit rate: 99.2%
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:17 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:17 [logger.py:49] Received request chatcmpl-39df2012-561c-4d9a-af1f-907cee124378: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:17 [async_llm.py:382] Added request chatcmpl-39df2012-561c-4d9a-af1f-907cee124378-828d1690.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:17 [encoder_cache_manager.py:126] mm_hash c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:17 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:17 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:17 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:17 [gpu_model_runner.py:871] Freeing encoder cache for 2281592003ec6a94b2722346f852b8ebcf2bb04d2100e872cdd0db54cf3ea828
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:17 [gpu_model_runner.py:2404] Encoder cache miss for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:17 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:17 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:17 [logger.py:49] Received request chatcmpl-ae6b8358-e188-45c9-b8f9-228f482522e4: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None), lora_request: None.
[0;36m(APIServer pid=2387499)[0;0m INFO:     127.0.0.1:49146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:17 [async_llm.py:382] Added request chatcmpl-ae6b8358-e188-45c9-b8f9-228f482522e4-80dac2e9.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:17 [encoder_cache_manager.py:126] mm_hash cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da is cached on CPU
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:17 [encoder_cache_manager.py:127] num_encoder_embeds: 256, num_free_slots: 144, num_freeable_slots: 400
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:17 [encoder_cache_manager.py:139] Evicting from GPU cache to free space for onboarding to GPU cache in GPU Model Runner: cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:17 [gpu_model_runner.py:868] free_encoder_mm_hashes: ['c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc']
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:17 [gpu_model_runner.py:871] Freeing encoder cache for c7cbe876c9abc6f421a53c240f92d4743603a6486cc37bf52942f7e934ba47bc
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:17 [gpu_model_runner.py:2404] Encoder cache miss for cc1ec63ccad91dc99465650b0248e769e4b2ffd62e72d39a8c8c8e8e452160da, loading from CPU.
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:17 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(EngineCore_DP0 pid=2387706)[0;0m INFO 01-21 21:33:17 [gpu_model_runner.py:868] free_encoder_mm_hashes: []
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:27 [loggers.py:263] Engine 000: Avg prompt throughput: 169.8 tokens/s, Avg generation throughput: 0.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 99.2%, Encoder cache hit rate: 99.2%
[0;36m(APIServer pid=2387499)[0;0m INFO 01-21 21:33:37 [loggers.py:263] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 99.2%, Encoder cache hit rate: 99.2%
